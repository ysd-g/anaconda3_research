{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.infiniteloop.co.jp/blog/2018/02/learning-keras-06/\n",
    "# https://www.ag.kagawa-u.ac.jp/charlesy/2017/07/21/keras%E3%81%A7%E5%8C%96%E5%90%88%E7%89%A9%E3%81%AE%E6%BA%B6%E8%A7%A3%E5%BA%A6%E4%BA%88%E6%B8%AC%EF%BC%88%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/yoshida/.pyenv/versions/3.6.6/lib/python3.6/site-packages')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.datasets import imdb\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.core import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "\n",
    "data = pd.read_csv(\"../src/Aiko_normalization_ver3_aaa.csv\")\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['No','width','height','seed_date','house','url', 'COL_21'], axis=1, inplace=True) #対象の列を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AveDiffTemp_7、AveSatu_7、monthを対象としたい時\n",
    "df.drop(['AveMinTemp_7', 'AveSatu_7', 'AveDiffTemp_7', 'AveMaxTemp_7','SumMaxTemp_7','SumMinTemp_7','SumDiffTemp_7'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2176, 7)\n"
     ]
    }
   ],
   "source": [
    "df['month'] = df['current_date'].str[:2] # \"11月28日\"とか\"4月3日\"の月日の先頭2文字だけ抽出\n",
    "df['month'] = df['month'].str.strip('月') # このままだと\"4月\"のように\"月\"が入っているので、\"月\"を削除する\n",
    "# 参考：https://deepage.net/features/pandas-str-extract.html\n",
    "# 参考：https://deepage.net/features/pandas-str-replace.html\n",
    "df['month'].isnull().sum() # 欠損値がないかを確認\n",
    "\n",
    "df.drop(['current_date'], axis=1, inplace=True) # current_dateはもう使わない\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 四半期を求める(4~6月:1st、7~9月:2nd、10~12月:3rd、1~3月:4th)(＊＊＊＊使うか使わないか＊＊＊＊)\n",
    "df['quarter'] = df['month'].replace({'4': '1st', '5': '1st', '6': '1st', \n",
    "                                     '7': '2nd', '8': '2nd', '9': '2nd', \n",
    "                                     '10': '3rd', '11': '3rd', '12': '3rd', \n",
    "                                     '1': '4th', '2': '4th', '3': '4th'\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダミー変数の作成\n",
    "df = pd.get_dummies(df, columns=['quarter'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2176, 7)\n"
     ]
    }
   ],
   "source": [
    "y = df[\"sweet_category\"] #目的変数\n",
    "df.drop(['sweet','sweet_category'], axis=1, inplace=True) #目的変数の列を削除\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['month'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>AveCO2_7</th>\n",
       "      <th>AveHum_7</th>\n",
       "      <th>AveTemp_7</th>\n",
       "      <th>quarter_3rd</th>\n",
       "      <th>quarter_4th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.54</td>\n",
       "      <td>614.28</td>\n",
       "      <td>66.84</td>\n",
       "      <td>21.14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.48</td>\n",
       "      <td>614.28</td>\n",
       "      <td>66.84</td>\n",
       "      <td>21.14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.68</td>\n",
       "      <td>614.28</td>\n",
       "      <td>66.84</td>\n",
       "      <td>21.14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size  AveCO2_7  AveHum_7  AveTemp_7  quarter_3rd  quarter_4th\n",
       "0  7.54    614.28     66.84      21.14            1            0\n",
       "1  6.48    614.28     66.84      21.14            1            0\n",
       "2  8.68    614.28     66.84      21.14            1            0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoshida/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by the scale function.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "df = preprocessing.scale(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.53104481,  0.79303369,  1.11169128, -0.46033908,  2.97424845,\n",
       "        -0.83904335],\n",
       "       [-0.04823753,  0.79303369,  1.11169128, -0.46033908,  2.97424845,\n",
       "        -0.83904335],\n",
       "       [ 1.15404658,  0.79303369,  1.11169128, -0.46033908,  2.97424845,\n",
       "        -0.83904335],\n",
       "       ...,\n",
       "       [-0.34334363, -1.33108582,  1.03852164,  0.20059694, -0.33621939,\n",
       "        -0.83904335],\n",
       "       [ 0.18128944, -1.33108582,  1.03852164,  0.20059694, -0.33621939,\n",
       "        -0.83904335],\n",
       "       [ 1.27973992, -1.33108582,  1.03852164,  0.20059694, -0.33621939,\n",
       "        -0.83904335]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2\n",
       "1       2\n",
       "2       1\n",
       "3       1\n",
       "4       2\n",
       "5       2\n",
       "6       2\n",
       "7       2\n",
       "8       2\n",
       "9       2\n",
       "10      2\n",
       "11      2\n",
       "12      3\n",
       "13      2\n",
       "14      3\n",
       "15      3\n",
       "16      3\n",
       "17      3\n",
       "18      3\n",
       "19      3\n",
       "20      3\n",
       "21      3\n",
       "22      2\n",
       "23      2\n",
       "24      2\n",
       "25      3\n",
       "26      3\n",
       "27      4\n",
       "28      4\n",
       "29      4\n",
       "       ..\n",
       "2146    3\n",
       "2147    2\n",
       "2148    2\n",
       "2149    1\n",
       "2150    2\n",
       "2151    3\n",
       "2152    4\n",
       "2153    2\n",
       "2154    2\n",
       "2155    3\n",
       "2156    2\n",
       "2157    1\n",
       "2158    4\n",
       "2159    2\n",
       "2160    3\n",
       "2161    4\n",
       "2162    2\n",
       "2163    3\n",
       "2164    1\n",
       "2165    3\n",
       "2166    3\n",
       "2167    2\n",
       "2168    3\n",
       "2169    3\n",
       "2170    2\n",
       "2171    2\n",
       "2172    3\n",
       "2173    4\n",
       "2174    3\n",
       "2175    2\n",
       "Name: sweet_category, Length: 2176, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y = np_utils.to_categorical(y)  # one-hotエンコード.例) 1 => [0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開発セットとテストセットに分割\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(df, y, test_size=0.3, random_state=0)\n",
    "#stratify は母集団のカテゴリの割合を保って分割するもので、目的変数を指定するのが一般的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練セットとテストセットに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.3, stratify=y_trainval, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力データ数＝入力層の数\n",
    "num_input = X_test.shape[1]\n",
    "\n",
    "# 分類するクラスの数\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, activation=\"relu\", input_dim=num_input)) #out_dim:10\n",
    "model.add(Dense(7, activation=\"relu\")) #out_dim:8\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy']) #'binaryではなくて'、categorical_crossentropyも気になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1066 samples, validate on 457 samples\n",
      "Epoch 1/600\n",
      "1066/1066 [==============================] - 0s 426us/step - loss: 1.5639 - acc: 0.2608 - val_loss: 1.5968 - val_acc: 0.2319\n",
      "Epoch 2/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 1.5500 - acc: 0.2627 - val_loss: 1.5833 - val_acc: 0.2363\n",
      "Epoch 3/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 1.5367 - acc: 0.2730 - val_loss: 1.5700 - val_acc: 0.2473\n",
      "Epoch 4/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 1.5237 - acc: 0.2795 - val_loss: 1.5568 - val_acc: 0.2538\n",
      "Epoch 5/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 1.5108 - acc: 0.2833 - val_loss: 1.5439 - val_acc: 0.2713\n",
      "Epoch 6/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 1.4985 - acc: 0.2964 - val_loss: 1.5315 - val_acc: 0.2735\n",
      "Epoch 7/600\n",
      "1066/1066 [==============================] - 0s 10us/step - loss: 1.4863 - acc: 0.3058 - val_loss: 1.5197 - val_acc: 0.2735\n",
      "Epoch 8/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 1.4745 - acc: 0.3068 - val_loss: 1.5080 - val_acc: 0.2779\n",
      "Epoch 9/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 1.4630 - acc: 0.3161 - val_loss: 1.4966 - val_acc: 0.2845\n",
      "Epoch 10/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 1.4517 - acc: 0.3265 - val_loss: 1.4854 - val_acc: 0.2998\n",
      "Epoch 11/600\n",
      "1066/1066 [==============================] - 0s 24us/step - loss: 1.4407 - acc: 0.3330 - val_loss: 1.4744 - val_acc: 0.3042\n",
      "Epoch 12/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 1.4299 - acc: 0.3386 - val_loss: 1.4637 - val_acc: 0.3107\n",
      "Epoch 13/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 1.4194 - acc: 0.3462 - val_loss: 1.4534 - val_acc: 0.3129\n",
      "Epoch 14/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 1.4093 - acc: 0.3527 - val_loss: 1.4434 - val_acc: 0.3260\n",
      "Epoch 15/600\n",
      "1066/1066 [==============================] - 0s 24us/step - loss: 1.3996 - acc: 0.3640 - val_loss: 1.4338 - val_acc: 0.3304\n",
      "Epoch 16/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 1.3903 - acc: 0.3705 - val_loss: 1.4246 - val_acc: 0.3370\n",
      "Epoch 17/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 1.3811 - acc: 0.3724 - val_loss: 1.4157 - val_acc: 0.3370\n",
      "Epoch 18/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 1.3723 - acc: 0.3734 - val_loss: 1.4070 - val_acc: 0.3479\n",
      "Epoch 19/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 1.3638 - acc: 0.3790 - val_loss: 1.3984 - val_acc: 0.3611\n",
      "Epoch 20/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 1.3554 - acc: 0.3818 - val_loss: 1.3900 - val_acc: 0.3611\n",
      "Epoch 21/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 1.3473 - acc: 0.3818 - val_loss: 1.3820 - val_acc: 0.3632\n",
      "Epoch 22/600\n",
      "1066/1066 [==============================] - 0s 26us/step - loss: 1.3395 - acc: 0.3912 - val_loss: 1.3742 - val_acc: 0.3589\n",
      "Epoch 23/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 1.3317 - acc: 0.3987 - val_loss: 1.3665 - val_acc: 0.3632\n",
      "Epoch 24/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 1.3241 - acc: 0.4071 - val_loss: 1.3589 - val_acc: 0.3676\n",
      "Epoch 25/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 1.3165 - acc: 0.4146 - val_loss: 1.3514 - val_acc: 0.3764\n",
      "Epoch 26/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 1.3090 - acc: 0.4212 - val_loss: 1.3439 - val_acc: 0.3786\n",
      "Epoch 27/600\n",
      "1066/1066 [==============================] - 0s 26us/step - loss: 1.3015 - acc: 0.4231 - val_loss: 1.3365 - val_acc: 0.3807\n",
      "Epoch 28/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 1.2941 - acc: 0.4278 - val_loss: 1.3293 - val_acc: 0.3917\n",
      "Epoch 29/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 1.2865 - acc: 0.4315 - val_loss: 1.3222 - val_acc: 0.3982\n",
      "Epoch 30/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 1.2791 - acc: 0.4381 - val_loss: 1.3150 - val_acc: 0.4048\n",
      "Epoch 31/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 1.2718 - acc: 0.4418 - val_loss: 1.3077 - val_acc: 0.4092\n",
      "Epoch 32/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 1.2646 - acc: 0.4456 - val_loss: 1.3004 - val_acc: 0.4136\n",
      "Epoch 33/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 1.2575 - acc: 0.4559 - val_loss: 1.2933 - val_acc: 0.4267\n",
      "Epoch 34/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 1.2503 - acc: 0.4737 - val_loss: 1.2864 - val_acc: 0.4354\n",
      "Epoch 35/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 1.2432 - acc: 0.4859 - val_loss: 1.2795 - val_acc: 0.4420\n",
      "Epoch 36/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 1.2360 - acc: 0.4953 - val_loss: 1.2726 - val_acc: 0.4508\n",
      "Epoch 37/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 1.2289 - acc: 0.5019 - val_loss: 1.2658 - val_acc: 0.4551\n",
      "Epoch 38/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 1.2218 - acc: 0.5028 - val_loss: 1.2590 - val_acc: 0.4530\n",
      "Epoch 39/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 1.2148 - acc: 0.5028 - val_loss: 1.2522 - val_acc: 0.4595\n",
      "Epoch 40/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 1.2077 - acc: 0.5038 - val_loss: 1.2452 - val_acc: 0.4705\n",
      "Epoch 41/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 1.2007 - acc: 0.5009 - val_loss: 1.2381 - val_acc: 0.4705\n",
      "Epoch 42/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 1.1935 - acc: 0.5047 - val_loss: 1.2310 - val_acc: 0.4858\n",
      "Epoch 43/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 1.1866 - acc: 0.5084 - val_loss: 1.2241 - val_acc: 0.4902\n",
      "Epoch 44/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 1.1795 - acc: 0.5131 - val_loss: 1.2171 - val_acc: 0.4923\n",
      "Epoch 45/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 1.1725 - acc: 0.5150 - val_loss: 1.2100 - val_acc: 0.4989\n",
      "Epoch 46/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 1.1657 - acc: 0.5216 - val_loss: 1.2031 - val_acc: 0.5033\n",
      "Epoch 47/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 1.1589 - acc: 0.5253 - val_loss: 1.1964 - val_acc: 0.5077\n",
      "Epoch 48/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 1.1524 - acc: 0.5310 - val_loss: 1.1899 - val_acc: 0.5098\n",
      "Epoch 49/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 1.1458 - acc: 0.5338 - val_loss: 1.1835 - val_acc: 0.5164\n",
      "Epoch 50/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 1.1393 - acc: 0.5375 - val_loss: 1.1770 - val_acc: 0.5230\n",
      "Epoch 51/600\n",
      "1066/1066 [==============================] - 0s 10us/step - loss: 1.1329 - acc: 0.5403 - val_loss: 1.1709 - val_acc: 0.5317\n",
      "Epoch 52/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 1.1265 - acc: 0.5403 - val_loss: 1.1648 - val_acc: 0.5427\n",
      "Epoch 53/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 1.1201 - acc: 0.5441 - val_loss: 1.1585 - val_acc: 0.5449\n",
      "Epoch 54/600\n",
      "1066/1066 [==============================] - 0s 10us/step - loss: 1.1137 - acc: 0.5488 - val_loss: 1.1520 - val_acc: 0.5558\n",
      "Epoch 55/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 1.1073 - acc: 0.5507 - val_loss: 1.1456 - val_acc: 0.5580\n",
      "Epoch 56/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 1.1010 - acc: 0.5525 - val_loss: 1.1393 - val_acc: 0.5602\n",
      "Epoch 57/600\n",
      "1066/1066 [==============================] - 0s 9us/step - loss: 1.0947 - acc: 0.5591 - val_loss: 1.1327 - val_acc: 0.5689\n",
      "Epoch 58/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 1.0884 - acc: 0.5666 - val_loss: 1.1259 - val_acc: 0.5711\n",
      "Epoch 59/600\n",
      "1066/1066 [==============================] - 0s 9us/step - loss: 1.0820 - acc: 0.5704 - val_loss: 1.1193 - val_acc: 0.5733\n",
      "Epoch 60/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066/1066 [==============================] - 0s 24us/step - loss: 1.0758 - acc: 0.5750 - val_loss: 1.1130 - val_acc: 0.5755\n",
      "Epoch 61/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 1.0696 - acc: 0.5769 - val_loss: 1.1068 - val_acc: 0.5733\n",
      "Epoch 62/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 1.0636 - acc: 0.5788 - val_loss: 1.1006 - val_acc: 0.5799\n",
      "Epoch 63/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 1.0576 - acc: 0.5835 - val_loss: 1.0945 - val_acc: 0.5864\n",
      "Epoch 64/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 1.0518 - acc: 0.5882 - val_loss: 1.0885 - val_acc: 0.5864\n",
      "Epoch 65/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 1.0460 - acc: 0.5863 - val_loss: 1.0825 - val_acc: 0.5864\n",
      "Epoch 66/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 1.0403 - acc: 0.5891 - val_loss: 1.0766 - val_acc: 0.5974\n",
      "Epoch 67/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 1.0347 - acc: 0.5938 - val_loss: 1.0704 - val_acc: 0.5996\n",
      "Epoch 68/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 1.0290 - acc: 0.5919 - val_loss: 1.0643 - val_acc: 0.6018\n",
      "Epoch 69/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 1.0234 - acc: 0.5947 - val_loss: 1.0582 - val_acc: 0.6018\n",
      "Epoch 70/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 1.0178 - acc: 0.5966 - val_loss: 1.0522 - val_acc: 0.6018\n",
      "Epoch 71/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 1.0121 - acc: 0.6004 - val_loss: 1.0458 - val_acc: 0.6018\n",
      "Epoch 72/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 1.0065 - acc: 0.6023 - val_loss: 1.0394 - val_acc: 0.6061\n",
      "Epoch 73/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 1.0007 - acc: 0.6051 - val_loss: 1.0331 - val_acc: 0.6127\n",
      "Epoch 74/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.9950 - acc: 0.6098 - val_loss: 1.0270 - val_acc: 0.6193\n",
      "Epoch 75/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 0.9890 - acc: 0.6088 - val_loss: 1.0210 - val_acc: 0.6193\n",
      "Epoch 76/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.9831 - acc: 0.6079 - val_loss: 1.0149 - val_acc: 0.6236\n",
      "Epoch 77/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.9772 - acc: 0.6116 - val_loss: 1.0088 - val_acc: 0.6258\n",
      "Epoch 78/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.9714 - acc: 0.6107 - val_loss: 1.0027 - val_acc: 0.6258\n",
      "Epoch 79/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.9656 - acc: 0.6116 - val_loss: 0.9968 - val_acc: 0.6214\n",
      "Epoch 80/600\n",
      "1066/1066 [==============================] - 0s 9us/step - loss: 0.9597 - acc: 0.6116 - val_loss: 0.9907 - val_acc: 0.6214\n",
      "Epoch 81/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.9538 - acc: 0.6116 - val_loss: 0.9846 - val_acc: 0.6302\n",
      "Epoch 82/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.9480 - acc: 0.6116 - val_loss: 0.9787 - val_acc: 0.6411\n",
      "Epoch 83/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.9424 - acc: 0.6135 - val_loss: 0.9727 - val_acc: 0.6433\n",
      "Epoch 84/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.9365 - acc: 0.6163 - val_loss: 0.9668 - val_acc: 0.6433\n",
      "Epoch 85/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.9307 - acc: 0.6144 - val_loss: 0.9608 - val_acc: 0.6368\n",
      "Epoch 86/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.9249 - acc: 0.6144 - val_loss: 0.9546 - val_acc: 0.6368\n",
      "Epoch 87/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.9192 - acc: 0.6238 - val_loss: 0.9482 - val_acc: 0.6433\n",
      "Epoch 88/600\n",
      "1066/1066 [==============================] - 0s 27us/step - loss: 0.9135 - acc: 0.6248 - val_loss: 0.9421 - val_acc: 0.6477\n",
      "Epoch 89/600\n",
      "1066/1066 [==============================] - 0s 29us/step - loss: 0.9079 - acc: 0.6229 - val_loss: 0.9362 - val_acc: 0.6499\n",
      "Epoch 90/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.9026 - acc: 0.6276 - val_loss: 0.9304 - val_acc: 0.6521\n",
      "Epoch 91/600\n",
      "1066/1066 [==============================] - 0s 9us/step - loss: 0.8970 - acc: 0.6332 - val_loss: 0.9245 - val_acc: 0.6521\n",
      "Epoch 92/600\n",
      "1066/1066 [==============================] - 0s 8us/step - loss: 0.8916 - acc: 0.6332 - val_loss: 0.9187 - val_acc: 0.6608\n",
      "Epoch 93/600\n",
      "1066/1066 [==============================] - 0s 7us/step - loss: 0.8863 - acc: 0.6360 - val_loss: 0.9130 - val_acc: 0.6608\n",
      "Epoch 94/600\n",
      "1066/1066 [==============================] - 0s 8us/step - loss: 0.8811 - acc: 0.6398 - val_loss: 0.9076 - val_acc: 0.6652\n",
      "Epoch 95/600\n",
      "1066/1066 [==============================] - 0s 9us/step - loss: 0.8761 - acc: 0.6417 - val_loss: 0.9023 - val_acc: 0.6696\n",
      "Epoch 96/600\n",
      "1066/1066 [==============================] - 0s 8us/step - loss: 0.8712 - acc: 0.6417 - val_loss: 0.8971 - val_acc: 0.6718\n",
      "Epoch 97/600\n",
      "1066/1066 [==============================] - 0s 9us/step - loss: 0.8665 - acc: 0.6435 - val_loss: 0.8918 - val_acc: 0.6761\n",
      "Epoch 98/600\n",
      "1066/1066 [==============================] - 0s 8us/step - loss: 0.8616 - acc: 0.6454 - val_loss: 0.8862 - val_acc: 0.6740\n",
      "Epoch 99/600\n",
      "1066/1066 [==============================] - 0s 6us/step - loss: 0.8568 - acc: 0.6473 - val_loss: 0.8806 - val_acc: 0.6740\n",
      "Epoch 100/600\n",
      "1066/1066 [==============================] - 0s 7us/step - loss: 0.8520 - acc: 0.6501 - val_loss: 0.8750 - val_acc: 0.6740\n",
      "Epoch 101/600\n",
      "1066/1066 [==============================] - 0s 9us/step - loss: 0.8475 - acc: 0.6473 - val_loss: 0.8696 - val_acc: 0.6718\n",
      "Epoch 102/600\n",
      "1066/1066 [==============================] - 0s 7us/step - loss: 0.8430 - acc: 0.6520 - val_loss: 0.8647 - val_acc: 0.6805\n",
      "Epoch 103/600\n",
      "1066/1066 [==============================] - 0s 6us/step - loss: 0.8386 - acc: 0.6595 - val_loss: 0.8598 - val_acc: 0.6805\n",
      "Epoch 104/600\n",
      "1066/1066 [==============================] - 0s 5us/step - loss: 0.8342 - acc: 0.6614 - val_loss: 0.8550 - val_acc: 0.6827\n",
      "Epoch 105/600\n",
      "1066/1066 [==============================] - 0s 7us/step - loss: 0.8298 - acc: 0.6614 - val_loss: 0.8502 - val_acc: 0.6827\n",
      "Epoch 106/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.8255 - acc: 0.6623 - val_loss: 0.8457 - val_acc: 0.6849\n",
      "Epoch 107/600\n",
      "1066/1066 [==============================] - 0s 6us/step - loss: 0.8215 - acc: 0.6651 - val_loss: 0.8414 - val_acc: 0.6871\n",
      "Epoch 108/600\n",
      "1066/1066 [==============================] - 0s 8us/step - loss: 0.8175 - acc: 0.6707 - val_loss: 0.8373 - val_acc: 0.6849\n",
      "Epoch 109/600\n",
      "1066/1066 [==============================] - 0s 7us/step - loss: 0.8137 - acc: 0.6707 - val_loss: 0.8330 - val_acc: 0.6849\n",
      "Epoch 110/600\n",
      "1066/1066 [==============================] - 0s 8us/step - loss: 0.8098 - acc: 0.6660 - val_loss: 0.8286 - val_acc: 0.6871\n",
      "Epoch 111/600\n",
      "1066/1066 [==============================] - 0s 7us/step - loss: 0.8059 - acc: 0.6698 - val_loss: 0.8243 - val_acc: 0.6827\n",
      "Epoch 112/600\n",
      "1066/1066 [==============================] - 0s 7us/step - loss: 0.8021 - acc: 0.6698 - val_loss: 0.8202 - val_acc: 0.6849\n",
      "Epoch 113/600\n",
      "1066/1066 [==============================] - 0s 33us/step - loss: 0.7983 - acc: 0.6698 - val_loss: 0.8163 - val_acc: 0.6827\n",
      "Epoch 114/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.7948 - acc: 0.6717 - val_loss: 0.8126 - val_acc: 0.6849\n",
      "Epoch 115/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.7915 - acc: 0.6717 - val_loss: 0.8089 - val_acc: 0.6827\n",
      "Epoch 116/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.7880 - acc: 0.6726 - val_loss: 0.8054 - val_acc: 0.6827\n",
      "Epoch 117/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.7848 - acc: 0.6735 - val_loss: 0.8019 - val_acc: 0.6849\n",
      "Epoch 118/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.7813 - acc: 0.6773 - val_loss: 0.7985 - val_acc: 0.6893\n",
      "Epoch 119/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.7779 - acc: 0.6811 - val_loss: 0.7953 - val_acc: 0.6849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.7746 - acc: 0.6792 - val_loss: 0.7918 - val_acc: 0.6849\n",
      "Epoch 121/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.7712 - acc: 0.6792 - val_loss: 0.7881 - val_acc: 0.6849\n",
      "Epoch 122/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.7678 - acc: 0.6792 - val_loss: 0.7844 - val_acc: 0.6893\n",
      "Epoch 123/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.7642 - acc: 0.6829 - val_loss: 0.7809 - val_acc: 0.6893\n",
      "Epoch 124/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.7608 - acc: 0.6829 - val_loss: 0.7778 - val_acc: 0.6980\n",
      "Epoch 125/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.7575 - acc: 0.6848 - val_loss: 0.7749 - val_acc: 0.7046\n",
      "Epoch 126/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.7544 - acc: 0.6904 - val_loss: 0.7723 - val_acc: 0.7090\n",
      "Epoch 127/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.7515 - acc: 0.6932 - val_loss: 0.7700 - val_acc: 0.7112\n",
      "Epoch 128/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.7487 - acc: 0.6951 - val_loss: 0.7672 - val_acc: 0.7112\n",
      "Epoch 129/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.7457 - acc: 0.6979 - val_loss: 0.7640 - val_acc: 0.7090\n",
      "Epoch 130/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.7427 - acc: 0.6970 - val_loss: 0.7609 - val_acc: 0.7090\n",
      "Epoch 131/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.7398 - acc: 0.6961 - val_loss: 0.7580 - val_acc: 0.7090\n",
      "Epoch 132/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.7370 - acc: 0.6979 - val_loss: 0.7552 - val_acc: 0.7133\n",
      "Epoch 133/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.7343 - acc: 0.6998 - val_loss: 0.7526 - val_acc: 0.7133\n",
      "Epoch 134/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.7317 - acc: 0.6989 - val_loss: 0.7500 - val_acc: 0.7155\n",
      "Epoch 135/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.7291 - acc: 0.6979 - val_loss: 0.7475 - val_acc: 0.7155\n",
      "Epoch 136/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.7264 - acc: 0.7017 - val_loss: 0.7450 - val_acc: 0.7177\n",
      "Epoch 137/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.7240 - acc: 0.7008 - val_loss: 0.7428 - val_acc: 0.7155\n",
      "Epoch 138/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.7215 - acc: 0.7017 - val_loss: 0.7405 - val_acc: 0.7133\n",
      "Epoch 139/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.7193 - acc: 0.6961 - val_loss: 0.7379 - val_acc: 0.7177\n",
      "Epoch 140/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.7170 - acc: 0.6970 - val_loss: 0.7352 - val_acc: 0.7177\n",
      "Epoch 141/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.7145 - acc: 0.6970 - val_loss: 0.7324 - val_acc: 0.7199\n",
      "Epoch 142/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.7121 - acc: 0.7008 - val_loss: 0.7294 - val_acc: 0.7221\n",
      "Epoch 143/600\n",
      "1066/1066 [==============================] - 0s 25us/step - loss: 0.7099 - acc: 0.7045 - val_loss: 0.7266 - val_acc: 0.7221\n",
      "Epoch 144/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.7079 - acc: 0.6979 - val_loss: 0.7241 - val_acc: 0.7265\n",
      "Epoch 145/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.7056 - acc: 0.7008 - val_loss: 0.7221 - val_acc: 0.7265\n",
      "Epoch 146/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.7040 - acc: 0.7064 - val_loss: 0.7203 - val_acc: 0.7221\n",
      "Epoch 147/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.7021 - acc: 0.7073 - val_loss: 0.7183 - val_acc: 0.7243\n",
      "Epoch 148/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.7001 - acc: 0.7073 - val_loss: 0.7164 - val_acc: 0.7221\n",
      "Epoch 149/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.6980 - acc: 0.7073 - val_loss: 0.7146 - val_acc: 0.7221\n",
      "Epoch 150/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.6960 - acc: 0.7120 - val_loss: 0.7130 - val_acc: 0.7243\n",
      "Epoch 151/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.6943 - acc: 0.7101 - val_loss: 0.7117 - val_acc: 0.7265\n",
      "Epoch 152/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.6927 - acc: 0.7139 - val_loss: 0.7106 - val_acc: 0.7287\n",
      "Epoch 153/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.6911 - acc: 0.7158 - val_loss: 0.7090 - val_acc: 0.7265\n",
      "Epoch 154/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.6891 - acc: 0.7139 - val_loss: 0.7068 - val_acc: 0.7243\n",
      "Epoch 155/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.6871 - acc: 0.7111 - val_loss: 0.7048 - val_acc: 0.7243\n",
      "Epoch 156/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.6854 - acc: 0.7064 - val_loss: 0.7029 - val_acc: 0.7243\n",
      "Epoch 157/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.6838 - acc: 0.7073 - val_loss: 0.7012 - val_acc: 0.7221\n",
      "Epoch 158/600\n",
      "1066/1066 [==============================] - 0s 26us/step - loss: 0.6822 - acc: 0.7120 - val_loss: 0.6993 - val_acc: 0.7243\n",
      "Epoch 159/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.6804 - acc: 0.7111 - val_loss: 0.6975 - val_acc: 0.7265\n",
      "Epoch 160/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.6787 - acc: 0.7120 - val_loss: 0.6958 - val_acc: 0.7265\n",
      "Epoch 161/600\n",
      "1066/1066 [==============================] - 0s 32us/step - loss: 0.6771 - acc: 0.7129 - val_loss: 0.6940 - val_acc: 0.7265\n",
      "Epoch 162/600\n",
      "1066/1066 [==============================] - 0s 26us/step - loss: 0.6754 - acc: 0.7111 - val_loss: 0.6922 - val_acc: 0.7287\n",
      "Epoch 163/600\n",
      "1066/1066 [==============================] - 0s 27us/step - loss: 0.6738 - acc: 0.7120 - val_loss: 0.6906 - val_acc: 0.7287\n",
      "Epoch 164/600\n",
      "1066/1066 [==============================] - 0s 24us/step - loss: 0.6722 - acc: 0.7111 - val_loss: 0.6889 - val_acc: 0.7309\n",
      "Epoch 165/600\n",
      "1066/1066 [==============================] - 0s 10us/step - loss: 0.6707 - acc: 0.7120 - val_loss: 0.6870 - val_acc: 0.7243\n",
      "Epoch 166/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.6692 - acc: 0.7083 - val_loss: 0.6850 - val_acc: 0.7330\n",
      "Epoch 167/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.6675 - acc: 0.7129 - val_loss: 0.6833 - val_acc: 0.7309\n",
      "Epoch 168/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.6661 - acc: 0.7129 - val_loss: 0.6821 - val_acc: 0.7243\n",
      "Epoch 169/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.6647 - acc: 0.7139 - val_loss: 0.6815 - val_acc: 0.7287\n",
      "Epoch 170/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.6635 - acc: 0.7176 - val_loss: 0.6810 - val_acc: 0.7265\n",
      "Epoch 171/600\n",
      "1066/1066 [==============================] - 0s 32us/step - loss: 0.6623 - acc: 0.7158 - val_loss: 0.6801 - val_acc: 0.7265\n",
      "Epoch 172/600\n",
      "1066/1066 [==============================] - 0s 27us/step - loss: 0.6610 - acc: 0.7148 - val_loss: 0.6789 - val_acc: 0.7265\n",
      "Epoch 173/600\n",
      "1066/1066 [==============================] - 0s 30us/step - loss: 0.6596 - acc: 0.7120 - val_loss: 0.6775 - val_acc: 0.7287\n",
      "Epoch 174/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.6584 - acc: 0.7158 - val_loss: 0.6758 - val_acc: 0.7309\n",
      "Epoch 175/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.6569 - acc: 0.7195 - val_loss: 0.6744 - val_acc: 0.7352\n",
      "Epoch 176/600\n",
      "1066/1066 [==============================] - 0s 8us/step - loss: 0.6556 - acc: 0.7205 - val_loss: 0.6731 - val_acc: 0.7309\n",
      "Epoch 177/600\n",
      "1066/1066 [==============================] - 0s 10us/step - loss: 0.6544 - acc: 0.7261 - val_loss: 0.6719 - val_acc: 0.7330\n",
      "Epoch 178/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.6533 - acc: 0.7242 - val_loss: 0.6709 - val_acc: 0.7309\n",
      "Epoch 179/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.6522 - acc: 0.7251 - val_loss: 0.6700 - val_acc: 0.7309\n",
      "Epoch 180/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.6511 - acc: 0.7214 - val_loss: 0.6693 - val_acc: 0.7265\n",
      "Epoch 181/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.6501 - acc: 0.7214 - val_loss: 0.6688 - val_acc: 0.7309\n",
      "Epoch 182/600\n",
      "1066/1066 [==============================] - 0s 27us/step - loss: 0.6493 - acc: 0.7214 - val_loss: 0.6685 - val_acc: 0.7330\n",
      "Epoch 183/600\n",
      "1066/1066 [==============================] - 0s 24us/step - loss: 0.6485 - acc: 0.7233 - val_loss: 0.6679 - val_acc: 0.7330\n",
      "Epoch 184/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.6477 - acc: 0.7242 - val_loss: 0.6671 - val_acc: 0.7330\n",
      "Epoch 185/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.6464 - acc: 0.7195 - val_loss: 0.6659 - val_acc: 0.7352\n",
      "Epoch 186/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 0.6450 - acc: 0.7176 - val_loss: 0.6649 - val_acc: 0.7352\n",
      "Epoch 187/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.6436 - acc: 0.7186 - val_loss: 0.6639 - val_acc: 0.7287\n",
      "Epoch 188/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.6422 - acc: 0.7176 - val_loss: 0.6630 - val_acc: 0.7287\n",
      "Epoch 189/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.6412 - acc: 0.7195 - val_loss: 0.6621 - val_acc: 0.7265\n",
      "Epoch 190/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.6401 - acc: 0.7176 - val_loss: 0.6609 - val_acc: 0.7287\n",
      "Epoch 191/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.6392 - acc: 0.7242 - val_loss: 0.6595 - val_acc: 0.7265\n",
      "Epoch 192/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.6384 - acc: 0.7242 - val_loss: 0.6582 - val_acc: 0.7177\n",
      "Epoch 193/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.6380 - acc: 0.7242 - val_loss: 0.6572 - val_acc: 0.7177\n",
      "Epoch 194/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.6373 - acc: 0.7242 - val_loss: 0.6563 - val_acc: 0.7177\n",
      "Epoch 195/600\n",
      "1066/1066 [==============================] - 0s 9us/step - loss: 0.6368 - acc: 0.7261 - val_loss: 0.6555 - val_acc: 0.7133\n",
      "Epoch 196/600\n",
      "1066/1066 [==============================] - 0s 10us/step - loss: 0.6354 - acc: 0.7223 - val_loss: 0.6547 - val_acc: 0.7133\n",
      "Epoch 197/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.6339 - acc: 0.7214 - val_loss: 0.6544 - val_acc: 0.7199\n",
      "Epoch 198/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.6329 - acc: 0.7205 - val_loss: 0.6540 - val_acc: 0.7287\n",
      "Epoch 199/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.6323 - acc: 0.7205 - val_loss: 0.6541 - val_acc: 0.7309\n",
      "Epoch 200/600\n",
      "1066/1066 [==============================] - 0s 26us/step - loss: 0.6316 - acc: 0.7214 - val_loss: 0.6541 - val_acc: 0.7309\n",
      "Epoch 201/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.6310 - acc: 0.7214 - val_loss: 0.6537 - val_acc: 0.7309\n",
      "Epoch 202/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.6305 - acc: 0.7186 - val_loss: 0.6534 - val_acc: 0.7265\n",
      "Epoch 203/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.6298 - acc: 0.7186 - val_loss: 0.6527 - val_acc: 0.7243\n",
      "Epoch 204/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.6289 - acc: 0.7214 - val_loss: 0.6513 - val_acc: 0.7243\n",
      "Epoch 205/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.6276 - acc: 0.7158 - val_loss: 0.6501 - val_acc: 0.7221\n",
      "Epoch 206/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.6263 - acc: 0.7214 - val_loss: 0.6492 - val_acc: 0.7155\n",
      "Epoch 207/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.6254 - acc: 0.7233 - val_loss: 0.6486 - val_acc: 0.7155\n",
      "Epoch 208/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.6248 - acc: 0.7233 - val_loss: 0.6480 - val_acc: 0.7199\n",
      "Epoch 209/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.6242 - acc: 0.7233 - val_loss: 0.6469 - val_acc: 0.7199\n",
      "Epoch 210/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.6235 - acc: 0.7214 - val_loss: 0.6461 - val_acc: 0.7177\n",
      "Epoch 211/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.6230 - acc: 0.7233 - val_loss: 0.6451 - val_acc: 0.7199\n",
      "Epoch 212/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.6224 - acc: 0.7242 - val_loss: 0.6439 - val_acc: 0.7177\n",
      "Epoch 213/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.6218 - acc: 0.7214 - val_loss: 0.6431 - val_acc: 0.7177\n",
      "Epoch 214/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.6212 - acc: 0.7214 - val_loss: 0.6428 - val_acc: 0.7221\n",
      "Epoch 215/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.6207 - acc: 0.7205 - val_loss: 0.6426 - val_acc: 0.7199\n",
      "Epoch 216/600\n",
      "1066/1066 [==============================] - 0s 24us/step - loss: 0.6204 - acc: 0.7176 - val_loss: 0.6426 - val_acc: 0.7199\n",
      "Epoch 217/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.6204 - acc: 0.7205 - val_loss: 0.6432 - val_acc: 0.7221\n",
      "Epoch 218/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.6208 - acc: 0.7233 - val_loss: 0.6433 - val_acc: 0.7199\n",
      "Epoch 219/600\n",
      "1066/1066 [==============================] - 0s 24us/step - loss: 0.6204 - acc: 0.7233 - val_loss: 0.6429 - val_acc: 0.7177\n",
      "Epoch 220/600\n",
      "1066/1066 [==============================] - 0s 30us/step - loss: 0.6193 - acc: 0.7214 - val_loss: 0.6418 - val_acc: 0.7199\n",
      "Epoch 221/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.6177 - acc: 0.7205 - val_loss: 0.6407 - val_acc: 0.7221\n",
      "Epoch 222/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.6164 - acc: 0.7223 - val_loss: 0.6401 - val_acc: 0.7199\n",
      "Epoch 223/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.6154 - acc: 0.7233 - val_loss: 0.6396 - val_acc: 0.7243\n",
      "Epoch 224/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.6153 - acc: 0.7233 - val_loss: 0.6393 - val_acc: 0.7221\n",
      "Epoch 225/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.6147 - acc: 0.7289 - val_loss: 0.6392 - val_acc: 0.7199\n",
      "Epoch 226/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.6141 - acc: 0.7298 - val_loss: 0.6393 - val_acc: 0.7177\n",
      "Epoch 227/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.6138 - acc: 0.7298 - val_loss: 0.6395 - val_acc: 0.7177\n",
      "Epoch 228/600\n",
      "1066/1066 [==============================] - 0s 32us/step - loss: 0.6136 - acc: 0.7308 - val_loss: 0.6398 - val_acc: 0.7177\n",
      "Epoch 229/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.6132 - acc: 0.7280 - val_loss: 0.6396 - val_acc: 0.7177\n",
      "Epoch 230/600\n",
      "1066/1066 [==============================] - 0s 29us/step - loss: 0.6127 - acc: 0.7270 - val_loss: 0.6393 - val_acc: 0.7199\n",
      "Epoch 231/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.6118 - acc: 0.7261 - val_loss: 0.6391 - val_acc: 0.7243\n",
      "Epoch 232/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.6111 - acc: 0.7261 - val_loss: 0.6387 - val_acc: 0.7243\n",
      "Epoch 233/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.6104 - acc: 0.7261 - val_loss: 0.6373 - val_acc: 0.7199\n",
      "Epoch 234/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.6096 - acc: 0.7251 - val_loss: 0.6360 - val_acc: 0.7221\n",
      "Epoch 235/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.6091 - acc: 0.7223 - val_loss: 0.6351 - val_acc: 0.7199\n",
      "Epoch 236/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.6086 - acc: 0.7233 - val_loss: 0.6342 - val_acc: 0.7177\n",
      "Epoch 237/600\n",
      "1066/1066 [==============================] - 0s 10us/step - loss: 0.6080 - acc: 0.7214 - val_loss: 0.6335 - val_acc: 0.7221\n",
      "Epoch 238/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.6074 - acc: 0.7233 - val_loss: 0.6325 - val_acc: 0.7199\n",
      "Epoch 239/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.6069 - acc: 0.7251 - val_loss: 0.6316 - val_acc: 0.7199\n",
      "Epoch 240/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.6066 - acc: 0.7214 - val_loss: 0.6307 - val_acc: 0.7221\n",
      "Epoch 241/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 0.6064 - acc: 0.7205 - val_loss: 0.6301 - val_acc: 0.7199\n",
      "Epoch 242/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.6061 - acc: 0.7214 - val_loss: 0.6294 - val_acc: 0.7221\n",
      "Epoch 243/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.6058 - acc: 0.7270 - val_loss: 0.6288 - val_acc: 0.7221\n",
      "Epoch 244/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.6052 - acc: 0.7233 - val_loss: 0.6285 - val_acc: 0.7199\n",
      "Epoch 245/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.6046 - acc: 0.7233 - val_loss: 0.6286 - val_acc: 0.7221\n",
      "Epoch 246/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.6041 - acc: 0.7223 - val_loss: 0.6289 - val_acc: 0.7265\n",
      "Epoch 247/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.6035 - acc: 0.7233 - val_loss: 0.6293 - val_acc: 0.7243\n",
      "Epoch 248/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.6034 - acc: 0.7205 - val_loss: 0.6301 - val_acc: 0.7155\n",
      "Epoch 249/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.6031 - acc: 0.7251 - val_loss: 0.6307 - val_acc: 0.7155\n",
      "Epoch 250/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.6029 - acc: 0.7270 - val_loss: 0.6312 - val_acc: 0.7112\n",
      "Epoch 251/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.6023 - acc: 0.7270 - val_loss: 0.6317 - val_acc: 0.7046\n",
      "Epoch 252/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.6019 - acc: 0.7289 - val_loss: 0.6320 - val_acc: 0.7068\n",
      "Epoch 253/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.6013 - acc: 0.7298 - val_loss: 0.6321 - val_acc: 0.7068\n",
      "Epoch 254/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.6008 - acc: 0.7317 - val_loss: 0.6315 - val_acc: 0.7155\n",
      "Epoch 255/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5999 - acc: 0.7326 - val_loss: 0.6310 - val_acc: 0.7243\n",
      "Epoch 256/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5996 - acc: 0.7345 - val_loss: 0.6307 - val_acc: 0.7199\n",
      "Epoch 257/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5994 - acc: 0.7345 - val_loss: 0.6301 - val_acc: 0.7243\n",
      "Epoch 258/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5994 - acc: 0.7336 - val_loss: 0.6297 - val_acc: 0.7287\n",
      "Epoch 259/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5995 - acc: 0.7373 - val_loss: 0.6293 - val_acc: 0.7287\n",
      "Epoch 260/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5992 - acc: 0.7355 - val_loss: 0.6286 - val_acc: 0.7243\n",
      "Epoch 261/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5985 - acc: 0.7326 - val_loss: 0.6283 - val_acc: 0.7265\n",
      "Epoch 262/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5975 - acc: 0.7383 - val_loss: 0.6284 - val_acc: 0.7221\n",
      "Epoch 263/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5969 - acc: 0.7402 - val_loss: 0.6286 - val_acc: 0.7309\n",
      "Epoch 264/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5968 - acc: 0.7373 - val_loss: 0.6287 - val_acc: 0.7287\n",
      "Epoch 265/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5977 - acc: 0.7336 - val_loss: 0.6293 - val_acc: 0.7287\n",
      "Epoch 266/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5987 - acc: 0.7308 - val_loss: 0.6297 - val_acc: 0.7287\n",
      "Epoch 267/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5991 - acc: 0.7317 - val_loss: 0.6294 - val_acc: 0.7309\n",
      "Epoch 268/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5982 - acc: 0.7317 - val_loss: 0.6282 - val_acc: 0.7287\n",
      "Epoch 269/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5968 - acc: 0.7345 - val_loss: 0.6269 - val_acc: 0.7265\n",
      "Epoch 270/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5957 - acc: 0.7364 - val_loss: 0.6257 - val_acc: 0.7265\n",
      "Epoch 271/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5950 - acc: 0.7364 - val_loss: 0.6248 - val_acc: 0.7243\n",
      "Epoch 272/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5945 - acc: 0.7355 - val_loss: 0.6241 - val_acc: 0.7243\n",
      "Epoch 273/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5943 - acc: 0.7345 - val_loss: 0.6239 - val_acc: 0.7221\n",
      "Epoch 274/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5946 - acc: 0.7364 - val_loss: 0.6243 - val_acc: 0.7155\n",
      "Epoch 275/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5949 - acc: 0.7364 - val_loss: 0.6251 - val_acc: 0.7221\n",
      "Epoch 276/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5952 - acc: 0.7364 - val_loss: 0.6259 - val_acc: 0.7243\n",
      "Epoch 277/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5952 - acc: 0.7345 - val_loss: 0.6266 - val_acc: 0.7177\n",
      "Epoch 278/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5948 - acc: 0.7383 - val_loss: 0.6268 - val_acc: 0.7177\n",
      "Epoch 279/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5940 - acc: 0.7430 - val_loss: 0.6270 - val_acc: 0.7221\n",
      "Epoch 280/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5929 - acc: 0.7439 - val_loss: 0.6273 - val_acc: 0.7243\n",
      "Epoch 281/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5925 - acc: 0.7439 - val_loss: 0.6280 - val_acc: 0.7287\n",
      "Epoch 282/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5919 - acc: 0.7448 - val_loss: 0.6285 - val_acc: 0.7309\n",
      "Epoch 283/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5921 - acc: 0.7458 - val_loss: 0.6296 - val_acc: 0.7287\n",
      "Epoch 284/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5925 - acc: 0.7458 - val_loss: 0.6309 - val_acc: 0.7199\n",
      "Epoch 285/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5925 - acc: 0.7402 - val_loss: 0.6312 - val_acc: 0.7221\n",
      "Epoch 286/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5923 - acc: 0.7402 - val_loss: 0.6310 - val_acc: 0.7199\n",
      "Epoch 287/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5918 - acc: 0.7411 - val_loss: 0.6301 - val_acc: 0.7199\n",
      "Epoch 288/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5914 - acc: 0.7364 - val_loss: 0.6289 - val_acc: 0.7199\n",
      "Epoch 289/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.5907 - acc: 0.7383 - val_loss: 0.6272 - val_acc: 0.7199\n",
      "Epoch 290/600\n",
      "1066/1066 [==============================] - 0s 26us/step - loss: 0.5902 - acc: 0.7430 - val_loss: 0.6258 - val_acc: 0.7155\n",
      "Epoch 291/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5899 - acc: 0.7420 - val_loss: 0.6249 - val_acc: 0.7199\n",
      "Epoch 292/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5898 - acc: 0.7420 - val_loss: 0.6243 - val_acc: 0.7199\n",
      "Epoch 293/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5896 - acc: 0.7411 - val_loss: 0.6238 - val_acc: 0.7199\n",
      "Epoch 294/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5893 - acc: 0.7439 - val_loss: 0.6233 - val_acc: 0.7177\n",
      "Epoch 295/600\n",
      "1066/1066 [==============================] - 0s 27us/step - loss: 0.5890 - acc: 0.7458 - val_loss: 0.6233 - val_acc: 0.7177\n",
      "Epoch 296/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5886 - acc: 0.7458 - val_loss: 0.6235 - val_acc: 0.7199\n",
      "Epoch 297/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5883 - acc: 0.7458 - val_loss: 0.6237 - val_acc: 0.7265\n",
      "Epoch 298/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5880 - acc: 0.7458 - val_loss: 0.6239 - val_acc: 0.7265\n",
      "Epoch 299/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5880 - acc: 0.7458 - val_loss: 0.6234 - val_acc: 0.7265\n",
      "Epoch 300/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5876 - acc: 0.7467 - val_loss: 0.6227 - val_acc: 0.7265\n",
      "Epoch 301/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5873 - acc: 0.7505 - val_loss: 0.6219 - val_acc: 0.7243\n",
      "Epoch 302/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5870 - acc: 0.7505 - val_loss: 0.6213 - val_acc: 0.7265\n",
      "Epoch 303/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5870 - acc: 0.7477 - val_loss: 0.6213 - val_acc: 0.7265\n",
      "Epoch 304/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5868 - acc: 0.7505 - val_loss: 0.6216 - val_acc: 0.7221\n",
      "Epoch 305/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5865 - acc: 0.7477 - val_loss: 0.6220 - val_acc: 0.7221\n",
      "Epoch 306/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5863 - acc: 0.7467 - val_loss: 0.6223 - val_acc: 0.7221\n",
      "Epoch 307/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5862 - acc: 0.7477 - val_loss: 0.6222 - val_acc: 0.7243\n",
      "Epoch 308/600\n",
      "1066/1066 [==============================] - 0s 10us/step - loss: 0.5861 - acc: 0.7420 - val_loss: 0.6219 - val_acc: 0.7265\n",
      "Epoch 309/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5859 - acc: 0.7411 - val_loss: 0.6210 - val_acc: 0.7243\n",
      "Epoch 310/600\n",
      "1066/1066 [==============================] - 0s 9us/step - loss: 0.5856 - acc: 0.7383 - val_loss: 0.6196 - val_acc: 0.7199\n",
      "Epoch 311/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 0.5852 - acc: 0.7486 - val_loss: 0.6181 - val_acc: 0.7155\n",
      "Epoch 312/600\n",
      "1066/1066 [==============================] - 0s 10us/step - loss: 0.5853 - acc: 0.7477 - val_loss: 0.6172 - val_acc: 0.7155\n",
      "Epoch 313/600\n",
      "1066/1066 [==============================] - 0s 9us/step - loss: 0.5852 - acc: 0.7495 - val_loss: 0.6165 - val_acc: 0.7177\n",
      "Epoch 314/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5848 - acc: 0.7505 - val_loss: 0.6158 - val_acc: 0.7221\n",
      "Epoch 315/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.5845 - acc: 0.7505 - val_loss: 0.6154 - val_acc: 0.7221\n",
      "Epoch 316/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5842 - acc: 0.7477 - val_loss: 0.6154 - val_acc: 0.7221\n",
      "Epoch 317/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5841 - acc: 0.7448 - val_loss: 0.6156 - val_acc: 0.7221\n",
      "Epoch 318/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5835 - acc: 0.7458 - val_loss: 0.6158 - val_acc: 0.7221\n",
      "Epoch 319/600\n",
      "1066/1066 [==============================] - 0s 8us/step - loss: 0.5836 - acc: 0.7420 - val_loss: 0.6158 - val_acc: 0.7221\n",
      "Epoch 320/600\n",
      "1066/1066 [==============================] - 0s 10us/step - loss: 0.5834 - acc: 0.7392 - val_loss: 0.6151 - val_acc: 0.7221\n",
      "Epoch 321/600\n",
      "1066/1066 [==============================] - ETA: 0s - loss: 0.5717 - acc: 0.732 - 0s 10us/step - loss: 0.5832 - acc: 0.7402 - val_loss: 0.6147 - val_acc: 0.7243\n",
      "Epoch 322/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5829 - acc: 0.7439 - val_loss: 0.6148 - val_acc: 0.7243\n",
      "Epoch 323/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5829 - acc: 0.7458 - val_loss: 0.6152 - val_acc: 0.7243\n",
      "Epoch 324/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5828 - acc: 0.7458 - val_loss: 0.6153 - val_acc: 0.7243\n",
      "Epoch 325/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5829 - acc: 0.7392 - val_loss: 0.6153 - val_acc: 0.7199\n",
      "Epoch 326/600\n",
      "1066/1066 [==============================] - 0s 9us/step - loss: 0.5828 - acc: 0.7420 - val_loss: 0.6156 - val_acc: 0.7199\n",
      "Epoch 327/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5826 - acc: 0.7430 - val_loss: 0.6161 - val_acc: 0.7155\n",
      "Epoch 328/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5823 - acc: 0.7439 - val_loss: 0.6159 - val_acc: 0.7155\n",
      "Epoch 329/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5821 - acc: 0.7448 - val_loss: 0.6152 - val_acc: 0.7155\n",
      "Epoch 330/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5818 - acc: 0.7439 - val_loss: 0.6144 - val_acc: 0.7177\n",
      "Epoch 331/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5814 - acc: 0.7458 - val_loss: 0.6134 - val_acc: 0.7243\n",
      "Epoch 332/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5811 - acc: 0.7514 - val_loss: 0.6124 - val_acc: 0.7243\n",
      "Epoch 333/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5812 - acc: 0.7514 - val_loss: 0.6118 - val_acc: 0.7243\n",
      "Epoch 334/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5813 - acc: 0.7467 - val_loss: 0.6116 - val_acc: 0.7243\n",
      "Epoch 335/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5815 - acc: 0.7467 - val_loss: 0.6120 - val_acc: 0.7221\n",
      "Epoch 336/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5815 - acc: 0.7467 - val_loss: 0.6122 - val_acc: 0.7221\n",
      "Epoch 337/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5813 - acc: 0.7514 - val_loss: 0.6117 - val_acc: 0.7221\n",
      "Epoch 338/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5810 - acc: 0.7458 - val_loss: 0.6115 - val_acc: 0.7243\n",
      "Epoch 339/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5807 - acc: 0.7439 - val_loss: 0.6111 - val_acc: 0.7265\n",
      "Epoch 340/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5801 - acc: 0.7467 - val_loss: 0.6109 - val_acc: 0.7243\n",
      "Epoch 341/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5797 - acc: 0.7439 - val_loss: 0.6109 - val_acc: 0.7243\n",
      "Epoch 342/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5797 - acc: 0.7448 - val_loss: 0.6106 - val_acc: 0.7243\n",
      "Epoch 343/600\n",
      "1066/1066 [==============================] - 0s 32us/step - loss: 0.5797 - acc: 0.7448 - val_loss: 0.6105 - val_acc: 0.7221\n",
      "Epoch 344/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5797 - acc: 0.7477 - val_loss: 0.6106 - val_acc: 0.7199\n",
      "Epoch 345/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5795 - acc: 0.7495 - val_loss: 0.6108 - val_acc: 0.7221\n",
      "Epoch 346/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5792 - acc: 0.7477 - val_loss: 0.6111 - val_acc: 0.7243\n",
      "Epoch 347/600\n",
      "1066/1066 [==============================] - 0s 29us/step - loss: 0.5791 - acc: 0.7448 - val_loss: 0.6112 - val_acc: 0.7243\n",
      "Epoch 348/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.5790 - acc: 0.7402 - val_loss: 0.6110 - val_acc: 0.7243\n",
      "Epoch 349/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.5790 - acc: 0.7430 - val_loss: 0.6112 - val_acc: 0.7287\n",
      "Epoch 350/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.5789 - acc: 0.7477 - val_loss: 0.6117 - val_acc: 0.7287\n",
      "Epoch 351/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5788 - acc: 0.7514 - val_loss: 0.6116 - val_acc: 0.7265\n",
      "Epoch 352/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5787 - acc: 0.7495 - val_loss: 0.6112 - val_acc: 0.7243\n",
      "Epoch 353/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.5784 - acc: 0.7505 - val_loss: 0.6111 - val_acc: 0.7243\n",
      "Epoch 354/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.5783 - acc: 0.7514 - val_loss: 0.6112 - val_acc: 0.7243\n",
      "Epoch 355/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5782 - acc: 0.7533 - val_loss: 0.6118 - val_acc: 0.7243\n",
      "Epoch 356/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5783 - acc: 0.7552 - val_loss: 0.6124 - val_acc: 0.7221\n",
      "Epoch 357/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5783 - acc: 0.7486 - val_loss: 0.6122 - val_acc: 0.7265\n",
      "Epoch 358/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 0.5779 - acc: 0.7505 - val_loss: 0.6113 - val_acc: 0.7309\n",
      "Epoch 359/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5775 - acc: 0.7505 - val_loss: 0.6101 - val_acc: 0.7287\n",
      "Epoch 360/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5770 - acc: 0.7523 - val_loss: 0.6089 - val_acc: 0.7287\n",
      "Epoch 361/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5770 - acc: 0.7523 - val_loss: 0.6085 - val_acc: 0.7287\n",
      "Epoch 362/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5768 - acc: 0.7523 - val_loss: 0.6090 - val_acc: 0.7243\n",
      "Epoch 363/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5764 - acc: 0.7505 - val_loss: 0.6096 - val_acc: 0.7287\n",
      "Epoch 364/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5763 - acc: 0.7448 - val_loss: 0.6107 - val_acc: 0.7265\n",
      "Epoch 365/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.5758 - acc: 0.7430 - val_loss: 0.6115 - val_acc: 0.7265\n",
      "Epoch 366/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.5759 - acc: 0.7430 - val_loss: 0.6131 - val_acc: 0.7287\n",
      "Epoch 367/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5765 - acc: 0.7458 - val_loss: 0.6143 - val_acc: 0.7330\n",
      "Epoch 368/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5767 - acc: 0.7448 - val_loss: 0.6147 - val_acc: 0.7352\n",
      "Epoch 369/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5767 - acc: 0.7439 - val_loss: 0.6146 - val_acc: 0.7287\n",
      "Epoch 370/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5766 - acc: 0.7430 - val_loss: 0.6141 - val_acc: 0.7287\n",
      "Epoch 371/600\n",
      "1066/1066 [==============================] - 0s 25us/step - loss: 0.5761 - acc: 0.7467 - val_loss: 0.6131 - val_acc: 0.7287\n",
      "Epoch 372/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5758 - acc: 0.7448 - val_loss: 0.6114 - val_acc: 0.7287\n",
      "Epoch 373/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5755 - acc: 0.7439 - val_loss: 0.6098 - val_acc: 0.7309\n",
      "Epoch 374/600\n",
      "1066/1066 [==============================] - 0s 25us/step - loss: 0.5751 - acc: 0.7477 - val_loss: 0.6095 - val_acc: 0.7265\n",
      "Epoch 375/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5745 - acc: 0.7495 - val_loss: 0.6098 - val_acc: 0.7265\n",
      "Epoch 376/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5742 - acc: 0.7505 - val_loss: 0.6099 - val_acc: 0.7287\n",
      "Epoch 377/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.5740 - acc: 0.7505 - val_loss: 0.6104 - val_acc: 0.7309\n",
      "Epoch 378/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5742 - acc: 0.7505 - val_loss: 0.6113 - val_acc: 0.7330\n",
      "Epoch 379/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5746 - acc: 0.7514 - val_loss: 0.6122 - val_acc: 0.7330\n",
      "Epoch 380/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5750 - acc: 0.7486 - val_loss: 0.6129 - val_acc: 0.7309\n",
      "Epoch 381/600\n",
      "1066/1066 [==============================] - 0s 24us/step - loss: 0.5754 - acc: 0.7486 - val_loss: 0.6137 - val_acc: 0.7330\n",
      "Epoch 382/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5758 - acc: 0.7458 - val_loss: 0.6140 - val_acc: 0.7287\n",
      "Epoch 383/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5757 - acc: 0.7467 - val_loss: 0.6135 - val_acc: 0.7309\n",
      "Epoch 384/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5755 - acc: 0.7495 - val_loss: 0.6130 - val_acc: 0.7287\n",
      "Epoch 385/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5755 - acc: 0.7477 - val_loss: 0.6126 - val_acc: 0.7243\n",
      "Epoch 386/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.5753 - acc: 0.7495 - val_loss: 0.6123 - val_acc: 0.7243\n",
      "Epoch 387/600\n",
      "1066/1066 [==============================] - 0s 29us/step - loss: 0.5749 - acc: 0.7495 - val_loss: 0.6119 - val_acc: 0.7265\n",
      "Epoch 388/600\n",
      "1066/1066 [==============================] - 0s 28us/step - loss: 0.5744 - acc: 0.7533 - val_loss: 0.6115 - val_acc: 0.7265\n",
      "Epoch 389/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5735 - acc: 0.7514 - val_loss: 0.6114 - val_acc: 0.7221\n",
      "Epoch 390/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5731 - acc: 0.7458 - val_loss: 0.6113 - val_acc: 0.7287\n",
      "Epoch 391/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5730 - acc: 0.7430 - val_loss: 0.6109 - val_acc: 0.7221\n",
      "Epoch 392/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5728 - acc: 0.7430 - val_loss: 0.6104 - val_acc: 0.7199\n",
      "Epoch 393/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5729 - acc: 0.7439 - val_loss: 0.6102 - val_acc: 0.7199\n",
      "Epoch 394/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5728 - acc: 0.7430 - val_loss: 0.6100 - val_acc: 0.7199\n",
      "Epoch 395/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.5726 - acc: 0.7439 - val_loss: 0.6094 - val_acc: 0.7221\n",
      "Epoch 396/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.5722 - acc: 0.7439 - val_loss: 0.6087 - val_acc: 0.7221\n",
      "Epoch 397/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5719 - acc: 0.7448 - val_loss: 0.6081 - val_acc: 0.7243\n",
      "Epoch 398/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5716 - acc: 0.7448 - val_loss: 0.6078 - val_acc: 0.7287\n",
      "Epoch 399/600\n",
      "1066/1066 [==============================] - 0s 25us/step - loss: 0.5713 - acc: 0.7477 - val_loss: 0.6077 - val_acc: 0.7287\n",
      "Epoch 400/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.5709 - acc: 0.7486 - val_loss: 0.6075 - val_acc: 0.7287\n",
      "Epoch 401/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5707 - acc: 0.7514 - val_loss: 0.6077 - val_acc: 0.7287\n",
      "Epoch 402/600\n",
      "1066/1066 [==============================] - 0s 10us/step - loss: 0.5705 - acc: 0.7523 - val_loss: 0.6080 - val_acc: 0.7265\n",
      "Epoch 403/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 0.5703 - acc: 0.7552 - val_loss: 0.6085 - val_acc: 0.7265\n",
      "Epoch 404/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5703 - acc: 0.7580 - val_loss: 0.6092 - val_acc: 0.7265\n",
      "Epoch 405/600\n",
      "1066/1066 [==============================] - 0s 25us/step - loss: 0.5705 - acc: 0.7561 - val_loss: 0.6096 - val_acc: 0.7287\n",
      "Epoch 406/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5708 - acc: 0.7561 - val_loss: 0.6096 - val_acc: 0.7287\n",
      "Epoch 407/600\n",
      "1066/1066 [==============================] - 0s 27us/step - loss: 0.5709 - acc: 0.7523 - val_loss: 0.6093 - val_acc: 0.7265\n",
      "Epoch 408/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5709 - acc: 0.7514 - val_loss: 0.6092 - val_acc: 0.7287\n",
      "Epoch 409/600\n",
      "1066/1066 [==============================] - 0s 26us/step - loss: 0.5707 - acc: 0.7542 - val_loss: 0.6090 - val_acc: 0.7309\n",
      "Epoch 410/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5704 - acc: 0.7533 - val_loss: 0.6083 - val_acc: 0.7309\n",
      "Epoch 411/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 0.5698 - acc: 0.7542 - val_loss: 0.6070 - val_acc: 0.7287\n",
      "Epoch 412/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.5692 - acc: 0.7542 - val_loss: 0.6055 - val_acc: 0.7287\n",
      "Epoch 413/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5691 - acc: 0.7523 - val_loss: 0.6040 - val_acc: 0.7287\n",
      "Epoch 414/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5692 - acc: 0.7533 - val_loss: 0.6030 - val_acc: 0.7287\n",
      "Epoch 415/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5693 - acc: 0.7552 - val_loss: 0.6024 - val_acc: 0.7265\n",
      "Epoch 416/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5692 - acc: 0.7533 - val_loss: 0.6022 - val_acc: 0.7265\n",
      "Epoch 417/600\n",
      "1066/1066 [==============================] - 0s 26us/step - loss: 0.5690 - acc: 0.7505 - val_loss: 0.6018 - val_acc: 0.7243\n",
      "Epoch 418/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5689 - acc: 0.7477 - val_loss: 0.6012 - val_acc: 0.7243\n",
      "Epoch 419/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5690 - acc: 0.7467 - val_loss: 0.6014 - val_acc: 0.7243\n",
      "Epoch 420/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5689 - acc: 0.7486 - val_loss: 0.6023 - val_acc: 0.7287\n",
      "Epoch 421/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5686 - acc: 0.7495 - val_loss: 0.6030 - val_acc: 0.7287\n",
      "Epoch 422/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5681 - acc: 0.7505 - val_loss: 0.6041 - val_acc: 0.7287\n",
      "Epoch 423/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.5677 - acc: 0.7589 - val_loss: 0.6060 - val_acc: 0.7309\n",
      "Epoch 424/600\n",
      "1066/1066 [==============================] - ETA: 0s - loss: 0.5574 - acc: 0.777 - 0s 17us/step - loss: 0.5682 - acc: 0.7552 - val_loss: 0.6083 - val_acc: 0.7330\n",
      "Epoch 425/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 0.5688 - acc: 0.7486 - val_loss: 0.6089 - val_acc: 0.7287\n",
      "Epoch 426/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5688 - acc: 0.7486 - val_loss: 0.6081 - val_acc: 0.7287\n",
      "Epoch 427/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5680 - acc: 0.7486 - val_loss: 0.6067 - val_acc: 0.7287\n",
      "Epoch 428/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5676 - acc: 0.7514 - val_loss: 0.6051 - val_acc: 0.7265\n",
      "Epoch 429/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5671 - acc: 0.7542 - val_loss: 0.6043 - val_acc: 0.7287\n",
      "Epoch 430/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5670 - acc: 0.7552 - val_loss: 0.6043 - val_acc: 0.7243\n",
      "Epoch 431/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.5671 - acc: 0.7523 - val_loss: 0.6041 - val_acc: 0.7265\n",
      "Epoch 432/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5670 - acc: 0.7533 - val_loss: 0.6035 - val_acc: 0.7309\n",
      "Epoch 433/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.5670 - acc: 0.7570 - val_loss: 0.6028 - val_acc: 0.7309\n",
      "Epoch 434/600\n",
      "1066/1066 [==============================] - 0s 29us/step - loss: 0.5670 - acc: 0.7598 - val_loss: 0.6028 - val_acc: 0.7309\n",
      "Epoch 435/600\n",
      "1066/1066 [==============================] - 0s 29us/step - loss: 0.5668 - acc: 0.7608 - val_loss: 0.6028 - val_acc: 0.7330\n",
      "Epoch 436/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5665 - acc: 0.7655 - val_loss: 0.6029 - val_acc: 0.7330\n",
      "Epoch 437/600\n",
      "1066/1066 [==============================] - 0s 26us/step - loss: 0.5663 - acc: 0.7655 - val_loss: 0.6030 - val_acc: 0.7352\n",
      "Epoch 438/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5660 - acc: 0.7664 - val_loss: 0.6032 - val_acc: 0.7352\n",
      "Epoch 439/600\n",
      "1066/1066 [==============================] - 0s 26us/step - loss: 0.5660 - acc: 0.7636 - val_loss: 0.6036 - val_acc: 0.7374\n",
      "Epoch 440/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5659 - acc: 0.7655 - val_loss: 0.6038 - val_acc: 0.7330\n",
      "Epoch 441/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5658 - acc: 0.7598 - val_loss: 0.6043 - val_acc: 0.7330\n",
      "Epoch 442/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5657 - acc: 0.7589 - val_loss: 0.6049 - val_acc: 0.7352\n",
      "Epoch 443/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5655 - acc: 0.7580 - val_loss: 0.6056 - val_acc: 0.7352\n",
      "Epoch 444/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5655 - acc: 0.7561 - val_loss: 0.6063 - val_acc: 0.7309\n",
      "Epoch 445/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.5653 - acc: 0.7514 - val_loss: 0.6072 - val_acc: 0.7352\n",
      "Epoch 446/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5654 - acc: 0.7477 - val_loss: 0.6080 - val_acc: 0.7352\n",
      "Epoch 447/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5659 - acc: 0.7495 - val_loss: 0.6080 - val_acc: 0.7330\n",
      "Epoch 448/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5658 - acc: 0.7486 - val_loss: 0.6067 - val_acc: 0.7309\n",
      "Epoch 449/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 0.5653 - acc: 0.7514 - val_loss: 0.6053 - val_acc: 0.7352\n",
      "Epoch 450/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5650 - acc: 0.7523 - val_loss: 0.6039 - val_acc: 0.7352\n",
      "Epoch 451/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5647 - acc: 0.7552 - val_loss: 0.6029 - val_acc: 0.7330\n",
      "Epoch 452/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5647 - acc: 0.7608 - val_loss: 0.6028 - val_acc: 0.7309\n",
      "Epoch 453/600\n",
      "1066/1066 [==============================] - 0s 24us/step - loss: 0.5648 - acc: 0.7627 - val_loss: 0.6029 - val_acc: 0.7287\n",
      "Epoch 454/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5649 - acc: 0.7598 - val_loss: 0.6027 - val_acc: 0.7287\n",
      "Epoch 455/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5649 - acc: 0.7617 - val_loss: 0.6024 - val_acc: 0.7265\n",
      "Epoch 456/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5647 - acc: 0.7608 - val_loss: 0.6020 - val_acc: 0.7287\n",
      "Epoch 457/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5642 - acc: 0.7636 - val_loss: 0.6018 - val_acc: 0.7287\n",
      "Epoch 458/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5636 - acc: 0.7655 - val_loss: 0.6022 - val_acc: 0.7265\n",
      "Epoch 459/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5635 - acc: 0.7608 - val_loss: 0.6027 - val_acc: 0.7199\n",
      "Epoch 460/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5641 - acc: 0.7570 - val_loss: 0.6032 - val_acc: 0.7199\n",
      "Epoch 461/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5644 - acc: 0.7533 - val_loss: 0.6029 - val_acc: 0.7177\n",
      "Epoch 462/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5647 - acc: 0.7533 - val_loss: 0.6028 - val_acc: 0.7177\n",
      "Epoch 463/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5644 - acc: 0.7570 - val_loss: 0.6032 - val_acc: 0.7177\n",
      "Epoch 464/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5639 - acc: 0.7552 - val_loss: 0.6036 - val_acc: 0.7199\n",
      "Epoch 465/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5637 - acc: 0.7542 - val_loss: 0.6039 - val_acc: 0.7221\n",
      "Epoch 466/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5635 - acc: 0.7533 - val_loss: 0.6041 - val_acc: 0.7221\n",
      "Epoch 467/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5634 - acc: 0.7477 - val_loss: 0.6043 - val_acc: 0.7243\n",
      "Epoch 468/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5632 - acc: 0.7477 - val_loss: 0.6036 - val_acc: 0.7243\n",
      "Epoch 469/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5629 - acc: 0.7486 - val_loss: 0.6025 - val_acc: 0.7243\n",
      "Epoch 470/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5626 - acc: 0.7533 - val_loss: 0.6015 - val_acc: 0.7265\n",
      "Epoch 471/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5623 - acc: 0.7552 - val_loss: 0.6005 - val_acc: 0.7265\n",
      "Epoch 472/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.5624 - acc: 0.7542 - val_loss: 0.6001 - val_acc: 0.7265\n",
      "Epoch 473/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5626 - acc: 0.7514 - val_loss: 0.6000 - val_acc: 0.7265\n",
      "Epoch 474/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5625 - acc: 0.7523 - val_loss: 0.6000 - val_acc: 0.7243\n",
      "Epoch 475/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5622 - acc: 0.7552 - val_loss: 0.6002 - val_acc: 0.7243\n",
      "Epoch 476/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5619 - acc: 0.7570 - val_loss: 0.6003 - val_acc: 0.7243\n",
      "Epoch 477/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5618 - acc: 0.7598 - val_loss: 0.6002 - val_acc: 0.7287\n",
      "Epoch 478/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5615 - acc: 0.7570 - val_loss: 0.5995 - val_acc: 0.7309\n",
      "Epoch 479/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5616 - acc: 0.7608 - val_loss: 0.5989 - val_acc: 0.7309\n",
      "Epoch 480/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5614 - acc: 0.7608 - val_loss: 0.5985 - val_acc: 0.7309\n",
      "Epoch 481/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5614 - acc: 0.7617 - val_loss: 0.5982 - val_acc: 0.7309\n",
      "Epoch 482/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5612 - acc: 0.7617 - val_loss: 0.5983 - val_acc: 0.7309\n",
      "Epoch 483/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5613 - acc: 0.7645 - val_loss: 0.5984 - val_acc: 0.7309\n",
      "Epoch 484/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5612 - acc: 0.7636 - val_loss: 0.5984 - val_acc: 0.7309\n",
      "Epoch 485/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5611 - acc: 0.7627 - val_loss: 0.5987 - val_acc: 0.7309\n",
      "Epoch 486/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5611 - acc: 0.7627 - val_loss: 0.5990 - val_acc: 0.7309\n",
      "Epoch 487/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5611 - acc: 0.7617 - val_loss: 0.5989 - val_acc: 0.7287\n",
      "Epoch 488/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5611 - acc: 0.7617 - val_loss: 0.5988 - val_acc: 0.7309\n",
      "Epoch 489/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5610 - acc: 0.7645 - val_loss: 0.5988 - val_acc: 0.7330\n",
      "Epoch 490/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5608 - acc: 0.7655 - val_loss: 0.5985 - val_acc: 0.7352\n",
      "Epoch 491/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5605 - acc: 0.7674 - val_loss: 0.5985 - val_acc: 0.7352\n",
      "Epoch 492/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5602 - acc: 0.7645 - val_loss: 0.5985 - val_acc: 0.7352\n",
      "Epoch 493/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5600 - acc: 0.7645 - val_loss: 0.5980 - val_acc: 0.7352\n",
      "Epoch 494/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5598 - acc: 0.7636 - val_loss: 0.5979 - val_acc: 0.7330\n",
      "Epoch 495/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5598 - acc: 0.7598 - val_loss: 0.5985 - val_acc: 0.7330\n",
      "Epoch 496/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5599 - acc: 0.7589 - val_loss: 0.5989 - val_acc: 0.7330\n",
      "Epoch 497/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5598 - acc: 0.7542 - val_loss: 0.5991 - val_acc: 0.7287\n",
      "Epoch 498/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5599 - acc: 0.7495 - val_loss: 0.5994 - val_acc: 0.7309\n",
      "Epoch 499/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5599 - acc: 0.7523 - val_loss: 0.5997 - val_acc: 0.7287\n",
      "Epoch 500/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5599 - acc: 0.7486 - val_loss: 0.6005 - val_acc: 0.7330\n",
      "Epoch 501/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5601 - acc: 0.7523 - val_loss: 0.6009 - val_acc: 0.7330\n",
      "Epoch 502/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5598 - acc: 0.7523 - val_loss: 0.6006 - val_acc: 0.7374\n",
      "Epoch 503/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.5595 - acc: 0.7570 - val_loss: 0.6003 - val_acc: 0.7374\n",
      "Epoch 504/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5592 - acc: 0.7589 - val_loss: 0.6001 - val_acc: 0.7374\n",
      "Epoch 505/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5590 - acc: 0.7589 - val_loss: 0.6003 - val_acc: 0.7396\n",
      "Epoch 506/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5593 - acc: 0.7580 - val_loss: 0.6009 - val_acc: 0.7396\n",
      "Epoch 507/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5598 - acc: 0.7552 - val_loss: 0.6014 - val_acc: 0.7396\n",
      "Epoch 508/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5603 - acc: 0.7523 - val_loss: 0.6014 - val_acc: 0.7440\n",
      "Epoch 509/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5605 - acc: 0.7552 - val_loss: 0.6018 - val_acc: 0.7440\n",
      "Epoch 510/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5604 - acc: 0.7542 - val_loss: 0.6023 - val_acc: 0.7440\n",
      "Epoch 511/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5603 - acc: 0.7533 - val_loss: 0.6023 - val_acc: 0.7462\n",
      "Epoch 512/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5595 - acc: 0.7552 - val_loss: 0.6017 - val_acc: 0.7462\n",
      "Epoch 513/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5589 - acc: 0.7627 - val_loss: 0.6005 - val_acc: 0.7418\n",
      "Epoch 514/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5583 - acc: 0.7655 - val_loss: 0.5994 - val_acc: 0.7396\n",
      "Epoch 515/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5579 - acc: 0.7664 - val_loss: 0.5988 - val_acc: 0.7374\n",
      "Epoch 516/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5576 - acc: 0.7627 - val_loss: 0.5990 - val_acc: 0.7309\n",
      "Epoch 517/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5576 - acc: 0.7589 - val_loss: 0.5998 - val_acc: 0.7309\n",
      "Epoch 518/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5576 - acc: 0.7552 - val_loss: 0.6010 - val_acc: 0.7330\n",
      "Epoch 519/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5576 - acc: 0.7542 - val_loss: 0.6024 - val_acc: 0.7352\n",
      "Epoch 520/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5577 - acc: 0.7542 - val_loss: 0.6031 - val_acc: 0.7352\n",
      "Epoch 521/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5577 - acc: 0.7570 - val_loss: 0.6028 - val_acc: 0.7352\n",
      "Epoch 522/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5575 - acc: 0.7598 - val_loss: 0.6017 - val_acc: 0.7330\n",
      "Epoch 523/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5574 - acc: 0.7645 - val_loss: 0.6006 - val_acc: 0.7374\n",
      "Epoch 524/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5574 - acc: 0.7674 - val_loss: 0.6001 - val_acc: 0.7374\n",
      "Epoch 525/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5575 - acc: 0.7674 - val_loss: 0.6001 - val_acc: 0.7352\n",
      "Epoch 526/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5579 - acc: 0.7664 - val_loss: 0.6003 - val_acc: 0.7352\n",
      "Epoch 527/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5579 - acc: 0.7683 - val_loss: 0.6002 - val_acc: 0.7352\n",
      "Epoch 528/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 0.5577 - acc: 0.7692 - val_loss: 0.6005 - val_acc: 0.7374\n",
      "Epoch 529/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5574 - acc: 0.7683 - val_loss: 0.6010 - val_acc: 0.7374\n",
      "Epoch 530/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5575 - acc: 0.7627 - val_loss: 0.6012 - val_acc: 0.7352\n",
      "Epoch 531/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5572 - acc: 0.7645 - val_loss: 0.6007 - val_acc: 0.7330\n",
      "Epoch 532/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5571 - acc: 0.7627 - val_loss: 0.6001 - val_acc: 0.7309\n",
      "Epoch 533/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5571 - acc: 0.7598 - val_loss: 0.6001 - val_acc: 0.7287\n",
      "Epoch 534/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5570 - acc: 0.7598 - val_loss: 0.6001 - val_acc: 0.7287\n",
      "Epoch 535/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5570 - acc: 0.7589 - val_loss: 0.6001 - val_acc: 0.7287\n",
      "Epoch 536/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5568 - acc: 0.7580 - val_loss: 0.6001 - val_acc: 0.7309\n",
      "Epoch 537/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5566 - acc: 0.7598 - val_loss: 0.5999 - val_acc: 0.7330\n",
      "Epoch 538/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5564 - acc: 0.7608 - val_loss: 0.5996 - val_acc: 0.7330\n",
      "Epoch 539/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5559 - acc: 0.7580 - val_loss: 0.5995 - val_acc: 0.7374\n",
      "Epoch 540/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5559 - acc: 0.7589 - val_loss: 0.5995 - val_acc: 0.7418\n",
      "Epoch 541/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5562 - acc: 0.7645 - val_loss: 0.5996 - val_acc: 0.7418\n",
      "Epoch 542/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5565 - acc: 0.7608 - val_loss: 0.5999 - val_acc: 0.7462\n",
      "Epoch 543/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5571 - acc: 0.7561 - val_loss: 0.6001 - val_acc: 0.7418\n",
      "Epoch 544/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5574 - acc: 0.7542 - val_loss: 0.5995 - val_acc: 0.7418\n",
      "Epoch 545/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5573 - acc: 0.7598 - val_loss: 0.5988 - val_acc: 0.7440\n",
      "Epoch 546/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5569 - acc: 0.7617 - val_loss: 0.5985 - val_acc: 0.7462\n",
      "Epoch 547/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5564 - acc: 0.7636 - val_loss: 0.5985 - val_acc: 0.7462\n",
      "Epoch 548/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5563 - acc: 0.7617 - val_loss: 0.5984 - val_acc: 0.7462\n",
      "Epoch 549/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5564 - acc: 0.7598 - val_loss: 0.5982 - val_acc: 0.7462\n",
      "Epoch 550/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5564 - acc: 0.7570 - val_loss: 0.5979 - val_acc: 0.7440\n",
      "Epoch 551/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5560 - acc: 0.7580 - val_loss: 0.5974 - val_acc: 0.7396\n",
      "Epoch 552/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 0.5556 - acc: 0.7580 - val_loss: 0.5965 - val_acc: 0.7352\n",
      "Epoch 553/600\n",
      "1066/1066 [==============================] - 0s 10us/step - loss: 0.5554 - acc: 0.7580 - val_loss: 0.5958 - val_acc: 0.7352\n",
      "Epoch 554/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5552 - acc: 0.7561 - val_loss: 0.5955 - val_acc: 0.7352\n",
      "Epoch 555/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5550 - acc: 0.7570 - val_loss: 0.5953 - val_acc: 0.7352\n",
      "Epoch 556/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5549 - acc: 0.7570 - val_loss: 0.5952 - val_acc: 0.7352\n",
      "Epoch 557/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5548 - acc: 0.7561 - val_loss: 0.5950 - val_acc: 0.7352\n",
      "Epoch 558/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5549 - acc: 0.7561 - val_loss: 0.5949 - val_acc: 0.7352\n",
      "Epoch 559/600\n",
      "1066/1066 [==============================] - 0s 12us/step - loss: 0.5548 - acc: 0.7570 - val_loss: 0.5951 - val_acc: 0.7352\n",
      "Epoch 560/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5550 - acc: 0.7580 - val_loss: 0.5950 - val_acc: 0.7330\n",
      "Epoch 561/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5549 - acc: 0.7570 - val_loss: 0.5947 - val_acc: 0.7330\n",
      "Epoch 562/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5548 - acc: 0.7570 - val_loss: 0.5942 - val_acc: 0.7352\n",
      "Epoch 563/600\n",
      "1066/1066 [==============================] - 0s 9us/step - loss: 0.5547 - acc: 0.7570 - val_loss: 0.5934 - val_acc: 0.7330\n",
      "Epoch 564/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 0.5541 - acc: 0.7580 - val_loss: 0.5924 - val_acc: 0.7330\n",
      "Epoch 565/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5542 - acc: 0.7542 - val_loss: 0.5916 - val_acc: 0.7330\n",
      "Epoch 566/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5545 - acc: 0.7514 - val_loss: 0.5912 - val_acc: 0.7309\n",
      "Epoch 567/600\n",
      "1066/1066 [==============================] - 0s 28us/step - loss: 0.5545 - acc: 0.7514 - val_loss: 0.5915 - val_acc: 0.7265\n",
      "Epoch 568/600\n",
      "1066/1066 [==============================] - 0s 11us/step - loss: 0.5548 - acc: 0.7523 - val_loss: 0.5921 - val_acc: 0.7287\n",
      "Epoch 569/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5549 - acc: 0.7505 - val_loss: 0.5920 - val_acc: 0.7330\n",
      "Epoch 570/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5547 - acc: 0.7514 - val_loss: 0.5915 - val_acc: 0.7352\n",
      "Epoch 571/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5547 - acc: 0.7514 - val_loss: 0.5910 - val_acc: 0.7352\n",
      "Epoch 572/600\n",
      "1066/1066 [==============================] - 0s 14us/step - loss: 0.5546 - acc: 0.7533 - val_loss: 0.5903 - val_acc: 0.7330\n",
      "Epoch 573/600\n",
      "1066/1066 [==============================] - 0s 23us/step - loss: 0.5540 - acc: 0.7514 - val_loss: 0.5896 - val_acc: 0.7418\n",
      "Epoch 574/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5536 - acc: 0.7561 - val_loss: 0.5897 - val_acc: 0.7396\n",
      "Epoch 575/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.5533 - acc: 0.7580 - val_loss: 0.5903 - val_acc: 0.7396\n",
      "Epoch 576/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5530 - acc: 0.7598 - val_loss: 0.5905 - val_acc: 0.7396\n",
      "Epoch 577/600\n",
      "1066/1066 [==============================] - 0s 20us/step - loss: 0.5528 - acc: 0.7589 - val_loss: 0.5910 - val_acc: 0.7396\n",
      "Epoch 578/600\n",
      "1066/1066 [==============================] - 0s 24us/step - loss: 0.5527 - acc: 0.7617 - val_loss: 0.5917 - val_acc: 0.7418\n",
      "Epoch 579/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5527 - acc: 0.7608 - val_loss: 0.5921 - val_acc: 0.7440\n",
      "Epoch 580/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5527 - acc: 0.7598 - val_loss: 0.5915 - val_acc: 0.7396\n",
      "Epoch 581/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5526 - acc: 0.7598 - val_loss: 0.5909 - val_acc: 0.7352\n",
      "Epoch 582/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5525 - acc: 0.7598 - val_loss: 0.5909 - val_acc: 0.7330\n",
      "Epoch 583/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5525 - acc: 0.7580 - val_loss: 0.5912 - val_acc: 0.7374\n",
      "Epoch 584/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5525 - acc: 0.7589 - val_loss: 0.5911 - val_acc: 0.7374\n",
      "Epoch 585/600\n",
      "1066/1066 [==============================] - 0s 22us/step - loss: 0.5526 - acc: 0.7598 - val_loss: 0.5905 - val_acc: 0.7374\n",
      "Epoch 586/600\n",
      "1066/1066 [==============================] - 0s 16us/step - loss: 0.5523 - acc: 0.7617 - val_loss: 0.5893 - val_acc: 0.7374\n",
      "Epoch 587/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5524 - acc: 0.7655 - val_loss: 0.5881 - val_acc: 0.7352\n",
      "Epoch 588/600\n",
      "1066/1066 [==============================] - 0s 18us/step - loss: 0.5521 - acc: 0.7664 - val_loss: 0.5875 - val_acc: 0.7330\n",
      "Epoch 589/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5519 - acc: 0.7627 - val_loss: 0.5874 - val_acc: 0.7330\n",
      "Epoch 590/600\n",
      "1066/1066 [==============================] - 0s 17us/step - loss: 0.5519 - acc: 0.7598 - val_loss: 0.5878 - val_acc: 0.7352\n",
      "Epoch 591/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5520 - acc: 0.7561 - val_loss: 0.5888 - val_acc: 0.7418\n",
      "Epoch 592/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5524 - acc: 0.7570 - val_loss: 0.5900 - val_acc: 0.7396\n",
      "Epoch 593/600\n",
      "1066/1066 [==============================] - 0s 25us/step - loss: 0.5525 - acc: 0.7542 - val_loss: 0.5907 - val_acc: 0.7374\n",
      "Epoch 594/600\n",
      "1066/1066 [==============================] - 0s 21us/step - loss: 0.5525 - acc: 0.7552 - val_loss: 0.5911 - val_acc: 0.7396\n",
      "Epoch 595/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5522 - acc: 0.7533 - val_loss: 0.5914 - val_acc: 0.7374\n",
      "Epoch 596/600\n",
      "1066/1066 [==============================] - 0s 19us/step - loss: 0.5521 - acc: 0.7552 - val_loss: 0.5915 - val_acc: 0.7396\n",
      "Epoch 597/600\n",
      "1066/1066 [==============================] - 0s 24us/step - loss: 0.5517 - acc: 0.7580 - val_loss: 0.5918 - val_acc: 0.7396\n",
      "Epoch 598/600\n",
      "1066/1066 [==============================] - 0s 13us/step - loss: 0.5517 - acc: 0.7542 - val_loss: 0.5920 - val_acc: 0.7374\n",
      "Epoch 599/600\n",
      "1066/1066 [==============================] - 0s 33us/step - loss: 0.5518 - acc: 0.7514 - val_loss: 0.5916 - val_acc: 0.7352\n",
      "Epoch 600/600\n",
      "1066/1066 [==============================] - 0s 15us/step - loss: 0.5516 - acc: 0.7495 - val_loss: 0.5909 - val_acc: 0.7330\n"
     ]
    }
   ],
   "source": [
    "# validation_dataに検証変数を入れることで、訓練データと検証データの学習サイクルが見える\n",
    "result = model.fit(X_train, y_train, epochs=600, batch_size=512, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = result.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd4VFX6wPHvSSeNVCAQSAKEFmoIRQGliyjYWMWylrWsKNafdVVs61rX3lbsFREVUUERAQGl9w5JKAkhvfd2fn+cYUggIQNkMgnzfp4nT+aeuffOe1Pue0+55yqtNUIIIQSAi6MDEEII0XxIUhBCCGElSUEIIYSVJAUhhBBWkhSEEEJYSVIQQghhJUlBCCGElSQFIYQQVpIUhBBCWLk5OoCTFRISoiMjIx0dhhBCtCjr16/P1FqHNrRei0sKkZGRrFu3ztFhCCFEi6KUOmDLetJ8JIQQwkqSghBCCCtJCkIIIawkKQghhLCSpCCEEMJKkoIQQggrSQpCCCGsJCkIIcQx/krIZP7Wwzjj44olKQgh7EJrza7UfH7akkJRWWWD69uyzoks3pXGPV9voqKq2qb1c4rKmbvxEFXVR0/87/6RQPdHF3DVzNXc9sUGluxOP62YTlZRWSXfbUimtKKqST+3JkkKQgi7mL0uiQmvLmf6lxuJefxXMgrKjlunulpTVa35fmMyMY//yvoDOTbvf8mudO6etZG9aQX8sSeDf3y8ju83HmLSGytIzik+4bbV1ZqL3/6Tu7/exE9bUgBIzy/l5YV7KKuspm94awD+8fE6th3Ks25XUVXN3bM2siU5F4Clu9PJLiq3OeaG3PHVRu6dvZnPV9l087FdSFIQQjQ6rTXfbzxUq2xlYlat5dKKKi555y+6/Gs+93y9GYDvNiQ3uN/DeSW8/Nsebvh4LXM3pTDulWVc9+EaIoO9uaBvGAkZhTw7f9cJ97MqMYsDWcWWzzRx/rojjfKqan675xx+uH0YXUJ9ALj503WUV1ZzMKuYz1YeYO6mFK79cA0/bDrE9R+t5cZP1tpcO6kpJbeEu2ZtZM2+bEorqkjJLeGPPRmASajV1Y5pumpxcx8JIZpGen4pReVVRIX41LuO1ppnF+xicr/29O7Q2lr+xeqDrErM5o7RXbljdDQDn/6N//2RQP/wADoFewPw6/ZUNiflWrcJ8fVk4Y40nr6oNy4uqs7P+8/8ncxcvg+AHu38uH1UVxIyClmzL5vbR3VlWNcQnpi3nU9W7mfZngzO6RaK1po3FscTn17IfeO70ynYm42Wz732rAg+X3WA9PxSFmw9TKcgb7q28UUpxUfXD2bW2oO8vTSBbo8uqBVHbnEF987eTHhgKzYezOUfH69lysBwRnZrg5+XGwWllWxOzqV3h9bsyyxkYERQre1/2XaYWz/fAMAPm1Ks5a3cXbltTBf++9seluxOZ0zPtqTnl5JVVE7PMP+GfmWNQpKCEC3U5qRcurfzw8vdtdH3XV2tmfDacrKLynnswl7cODzK+t62Q3l0a+uHh5sLKXmlvLcskfeWJbL/uQvIK6kgKbuYNxfHExcRyF1jonFzdeHFv/XjgTmbue6jNSy4awRe7q6sSsyilbsrr1zRn3O7hbJwRyp3zdrExNeXE+rnyfvXxeHp5kpGQRk5xeW4KGVNCCO7h/LxDYPrjP2O0V2Zv/UwM5cnck63UBbuSOPl3/YAsD+riBkX9mLH4Xw6BLTiurMj+XTlAWb8sJ2/ErJ46PweKGUSUqdgb+4/rzudgrx5f8U+4tMLAQj28SCrqJyqas3TF/fm4z/388eeDJbvzQTA38sNN1eXWs1K86YPo294AACH80q4f84WAO4/rzv+rdz5eu1B0vPLePny/gzpHMQbS+K58ZN13DG6Kx+s2EdxeRW7np5gl9/1sSQpCNECzd96mNu+2MD/jevG+Jh2pOaXMrRzEJ5utp80ftycgq+XG6O6tznuvQXbUq0ntad/2sHzC3ZRXqOJ5IK+Ybx1VSx70wqsZXnFFZz70hJyiysAeGJyL9xcTQv1hN7tcHdV3PjJOpbuzuCcbiEs2pnO8OgQJvRuB8DYnm1xdVHsSi1gV2oBF735Jx9eP4hrPlhNYkYRAIHe7tw2sit/iwuv97iCfT25eEAHZi5PZGVCFq/8tocgHw/uHdeNR+duY8q7K80x9AmjS6gvgyOD+GV7KgHe7lwzNKLWvpRSTB3cib/FdaS8shqNxs3FxVpziIsIpFeYP3/GZ+KiFN9uSEYpRXllFf06BoCG/y1L5LOVB7hhmAuBPu6c88ISKqo0S+4baa2FXR4XjkLh4WZ+Xo9d2IvH5m7jjcXxNX4nh7lkQP3H3VhUSxtyFRcXp2XqbNHcaK15a0k8iZlF9Arz58bhUdYrzs1JuXyzPomL+3cgLjKozu3j0wuY+t4qLugThpurCxVV1Tx1UW/AjEgpLq8i1M+Td/9I4LkFu/Bwc6G8snY79oSYdrz794G1ysorq/lgxT62HsolMaOIz24cglKQWVjGhFeXA/DXQ6NpH9DKuk1CRiGT3lhBu9ZevHbFACa9uQKA9q29SMkrta730x3DWbgjjdd/3wvA5H7tmbfZNIV0DvVhwV0jaiWpyqpqBj2ziB7t/CmpqGJTUi5f3TyUs7oEW9d5bdFeft6awqWx4Ty3YBfDu4awIj7T+v6jF/TkphGdG/x9rErMYup7q6zLl8eF8++L+9D3yV8JaOVBpyBvXvxbXyKCfcgqLGPW2iQGdAzg7K4hDe77yM9o1+ECLugb1uC6D327hVlrk2qVubko4v8z8YTbbTuUx4wfthEZ7MPm5Fy0hvmWWtapUEqt11rHNbieJAUhTt/mpFwueutP6/KEmHY8eH4PnvpxO0t2m87DzqE+/HbPubjW0V7+ztIEnv+ldufokvtG0j7AiwmvLudwXglL7xvF0Gd/ByCmvT9p+WVkFpYxOCqIXmH+fPzXftY8MoY2fl5orckuKuelhbv5ak0SSkF9/+rPX9aHKwZ1AqC4vJK/f7CGHSn5/HL3CCKCffhjTwZlFVWMj2lHWn4pnm4ujHhhCQWlZghpeGArknNKABgcFcQ7V8fi6+VWZ63lnq83WTug7xvfjemjo+v9mY54YTFJ2SX4eLjy6Y1DSMgo5LLY8Dp/fseqqKom+pGj/QBHml5yi8vxcHPB2xVwcQXV8L5OqMrUinB1r3eV9IJS3l++jznrk4mLCKRzqC9je7ap9wKhLsv2ZHDth2t4YEJ3bhvZ9ZRClaQgRCMoKqvktx1pvLM0gdiIAJ69tK/1vczCMqZ9vp6U3FKyisooq6wmPLAVSdnmBDm2Z1sW7UwDYMaFvXjqpx24uig2PDqOJ3/czs7UAr66eQjL9mZy51cbG4ylT4fWbD2Ux11jopk+uit70grYlJTL1UMi2JSUy8Vv/cnkfu1JzS8lPKAV31lOvjcMi+TxSTG8vzyRf/+807q/96+N48Fvt3B21xD+NbEHOUUVPPXTdlYlZvPCZX25fFDHemOZuSyRZ+bv5II+Ybx8RT8+/esASTnFXD0kgu7t/OrdbuPBHK79cA3nxbTjmUt6n7C567VFe/ls1QHuGRfN1UMi6l2vPmn5pfy2I42+4a2t7fkUZsCeBTD/Aeh/JVz4yknvl2UvwuZZ0O9KWPxvk1jGPwNn3Xby+yrJAeUCXq0bXHXJLtPc5u56aoNGJSkIcZoyC8u44PXlpOWb8fWuLoqVD49mxd5M5m9NZfGuNNxcXJjUrz1uLopLYzswpHMwxeWVTHpjBQmWdvDXrxzAhX3COPelJSRllxAXEci6OsbjPz6pFzHtW1NUVskNH6+1lo/p0YZAHw/mrDfDNTfPGE9r79pXppVV1Qx7frE11pr+uH8kEcGm7Xr2uiR8PNyszR4zftjGpytrj4mf3K89r185oMGfz+7UAjqH+pzySarJaQ1vxkGWpZ3ewxceSAQ3T9v3kZcMr8QcXe4y2tQWktfCfXvBy4YRQoe3gLs3tO4A7w4H//Zw3Y+118lOhJJc6BBre2wNsDUpSEezcFrbDuXxztIEnr64N0E+Hse9v2BbKmn5ZTxzSW9CfT255bP1DH7m91rrXHd2BI9c0KtWmbeHG3eOieauWZsA6BTkjYuLYtn9o5jw6nLWHcip1T7/r4k9iIsMYkDHAGs/xIbHxpGSW4K/lzudgr1ZuD2VOeuT6dHO77iEAODm6sLMa+P4+M/9XBobzup9WZzdJYQgHw9rQgC4PK721f/00V0pKqsir6ScAZ0CmdS3vXXIaENOVCNodvJTYOlzJiEMuxs6DITZf4e178NZt9u+n18ePvraPxwu+wAydsNHEyD+N+h9WcP7+N8I8z3qXBNPVgIUpoNvjQ7/zy6BnP0w4O+mJtFxCAy42vY4T4PUFITTScwo5K0lCSzcnkpBWSW3jezCAxN6sCMln62HcunRzp+NB3N4e2kCXu6u/HH/SMoqq3nk+218a7m56l8Te9C/YyD9OrautwlkwqvL2JVawMbHxhFoSToLt6dy3zebef6yvsxam4SvpxtvXd3w1WBpRRX/mb+Tm4Z3tvmk3SxUV8PP90LkcOgzxTEx/Hwf7PwRirMgJBqu/xlaBcKXV8D+FXDnRvBra07M31wPUeeATwhs+w4mvghtLTWDihJ4oTP0uBAmvWb6JNw8oboK/tsdIkfA3z46cSyVZfBvy8nfLww6DYXt30NwNHh4g6c/XPAyvDXo+G2v+wmiRpzyj0Gaj4R9VZRCZYn55wIoL4J3zoaz74C4G01Z6lb4/lYoK4Dpa8Hdy3HxYkYIvfjrbt5emlCrPMTXk2+nncXE15ZTVF57zpl/ntuZh8/vaV1++qcd/LItlQV3j8Dfq/7ORTD9EdsO5TGkc3Ctcq21tUZQ83WjKMmFmaNh2F0w8Lra5RXF5qrTr13jfR5AVaVpG/cNrV2enwIfX2CaQsA0mVQUg29b8PCB0ny4di6063P6MZTkwIcTIGMXBHWGS/4HYf0gfSe8dy606wujH4Nu449uk7LJvDfoZtj6DZTmHr/fdn3h5iXg6gbrPoSf7oFrvoOuY2qvN+9O2PDJ0WXlAgERJnFMeg06DoWE3+HLy837l86EvpebJq1XekN+MgRGQUk2lFqm1eh2vvl5RY6AjZ+Zzuxb/zzl/yNJCsJ+cpPgqyshbSs8sA+8g2DHPFMdB+g8CryDYduco9vc9DuEH/P3mLYdDq03f/RBUdiqqlqzel8WcRFB1nHdDdFa86/vt/HVmoPWMncqebpfNo9tDmKgyx42uvTiycl9WLY3g2uGRPDdxkPcf1532uosk9ja9EBrTbXGphEwTSLxD2g/4Ghb9tLnYOmz5srzliVwaIO50n17KBSZUVBMfhMGXGP7yButYe9v4B9W9wn8yytgzy/wr8Pmarck11yZ//oIlOWZq9+y/Lr33T4WblpkTp62SlgCeTWGeLq4QfzvsP070xS0+n9QVQ6dzoKD5p4Epq8ztYSaqirhP2Fm3dYdzT7dWsG4p6Cy1Pxd/3C76UTuOAQ+GAteAabvwO2Y5sbcJNj0JWjLRUXyOpMEjug6DrITjibIW/+EdmbIMStehUWPw+WfmYurubeaz7lzo4kBzPF9fimMfRKG3237z6oGSQri9CUsgepKiB53tCxpDXxQY9mvPfzjF/j2JkheU3v7Pn+DflPh88tM++vZ02HwLeYEkJsEbw0+eiV0/U82hfT7zjSmf7mRkooqpo3swoMTejS4jdaax+dt59OVB7hpeBQPt99EVVYCu4p86bvpCbJUEME6m5Xd7uesqx6tvXFFCTwbbn4O9+4yJ0ZHObgKdv0EZ003V/tZCfBGLHiHQK+LoE1PczWbvgM8W5sry+JMCB98/O/m4nfN6Js9v5oO18hhpnzvb+YE7+kHg/8JGz6Fw5tg93xQrjDweug42PxeD2+BNe+Zq1iAmEtMLSBnv9mHhx9M+QA6j4R9y0wn7U93w/B7YMUrMPJhk8CizoE2Mebv7Ngr8GPt+hlmXVX3e2dNh/Oegcy98PU1ptYApg9h3JN1b/P5ZRC/CK751sSgq6C15QYxreGrqSZ237bmCv6ab23r/K2qhH1/mN/Tpi9h5Zum/MJXoX1/k8iP0Nok7w6xJlEnrQH/DqYjuqYt30CPiaaWdQokKYiTt+xF2P+n6TwrLzRXl9VVMO0vM2Tux7vMP0h5QZ2b64ve5t2sfkxbYTnB3LrCXFnOHAOHLL+zC19lVvUYhmV/R8dVj0PMpbBjrjnh+rVtMMQp7/zFgexisi3TDIzv1ZZe7f259dwulpuyylm3P5u3F8czv8u3lHUexyWLWrM7rYDrz47k8fEdUc/V7mzVnv6oI1eyE56HobcefXP7XPjG0gwz6TVzUqypqhK+u8mc8ArTTJIc9S9z9br5S5j431Or7q+ZaWonI+49Wvb+WDPKpcto04Tx1+vw24zjt+02wZxkimpM+zziPjj3Qfj+n+aK2sPX1NwSl5r3n8gzJ6dX+5rtqsrNFf6RJhUPPwjoaI6zLB9ir4WCNHM13HGIqR2kbz/6eVHnwNSvwNO3dmwVpebnUV4M7q1g7m3m5wTg6mmujmueDIuzYd4d5mfb6SyToHKT4Lp5pokGYO9C0y5/5ayjJ8z4ReaEHxBh/n6PjeOIynKoKDraDHqs3CSTGEpyYfLrDSetuhRnwwuWmvD9Caa/wgEkKYiTU5QFL9a4U9TT3yQEFzeTBHS1ed1tAkSPhx/vPLpuxyEQMYzEvvcy+uVlxKo9PBixh8Ud72BDUi6f3zgYz2XPwvKX+E0P5uayu3nG4xOubrUSblkKbw0B5cKK0d8wPz2IorJKtqfk884lEYS3a8cbv+8kJjyY7tv+S8ru9fx11jvcObYnKz6ZwfBDH/BQ+Y38XD0Uf5cyKqrBlWq6qyRmez4NQHTpp9w1vhe3RaTg8tnk44/9iTzzz//5ZZC524w/19WmE7EoCw7+Zar1/abC5Ddqb5u61QwrDO1x9Mo0cgQc+MtcdY59Es6+0zQdVFeaE+/CR0wHp1dr+OcyMyRx7u2wdbZpNjjnflhwv9lXqyAY8xgsf9k0b/i0MSdtF3eorjCfe8XnZqglwJBp5mr5SHPMb4+b9vbznzcnYTBX6YueqH0cj6abq/vZ15rmJVcP2DILwgeZDtqht0FwF3My//qao00jQ6bB+c+Z1zVrkX/7BGIutuUvz4wAyt5nrqYvess0bVVVmHb6rbPNOmH9j15YDL8Xxj5u276bi+UvQ2CEbaOT7ESSgrBdeTG8MRAKUmqXn/esOSmsfd+cIIbfA2OfMCe2DZ9AaE9zwuhhbtev665cgM9vHEK71p4kvHExMZ7pXOnxGs8WPEobrypucH2Wy1vvZFrqYyyt7s+86hGkVvvTRuXytsfrACRUh5FJa4a4mH0fGjKDDhPuhdf7m6aKBowte4FFz/4TPr0YEpeYwiu+gK+vNm3J92wzZWWF8MNtsOOH2jsY+S9Y+h/zeujtpprv3sqMJCnKgAUPwB0bTDvy7p8t2ytzJdu2F8T9w3RQ1tR3qvmZ9roIpnwMz3WC0O6mc7YgxSSHwbeYk2LOfpNAhtxqOkV3zDXDINfOPNo0sukrMzKltQ1z45TkwKp3TQLc/n3tePzaw/Q1pvmoPtXVsP4jKDhsjs2//dH3cpNM7eNk+iyO7PP5CHPivHkpfDoZDvwJ3Seaz+gy2jSNFabD4JtrD98UNpGkIBq280czImTZS+ZqePwz0PNCc9dnRZEZR31kfoTDm81IDJfaHbsHsorYnVpA/44BDH9+CZ1DfXht6gA+W7WfUF8v3loSb51I7QG3WdzqPh9GPULp8tdZVNmfO0tuBuC/7u9wmety2+J29zHt15s+NyfL0rw6VysJG0Krw6tZEfsKwyf/A2Zdbdrke10Ml39iOv08fGufYI6tMYHpTF/4mPm8uvi0gfv2mJ9Vab5JPIGRphP4t8cgoJO5sh/zmFn/yFDE1/pDzj4zBPHne+Git83JL2mV6Qdo3cE0531+qVnn2HHqh9ZDm15HawAnq7radMT+8hCkboGQ7vD3749vy24qRzqs2w+AlI0m4Y2ZcXKd0KJekhRE/bSGhY8e7fyCo510NSRmFPLUTzt4cEKPeudyH/niEvZnFdMpyJuD2cUsuvccurY5epX5+aoDPPbDNrSGke47+cjnddSRk/jEl0jr8XdeXrgHj40f8LT7x7X2ndfzSq5JHMf70atoqzNh0qsmAXx+GeQfNh2eU780V+uzrjTH0OMC08wFpvnr2Q7mqvv+BDNktnUHuOobM8SwPk9YphzwCzMn6YvfNn0HVWWmbft1SyfhsLtMJ+LEl+puKslOPLru6EdNs1BNRZnwYheTmKvK4Y71Jpkcq6Lk1E/8tqgsN0MiW3c68c/F3irLTL/Vzp+g+/lw6XunPzeRsJKkII6Xd8hU9T+aaGoGR8aNu3rAYxm1Vt12KI8rZ66ioLSSHu38ePOqAfyyLZVVidl0CvYmPLAVi3em15quIdDbnY0zxh/7qbWVFZjRPAA3LzZ3lgIL/lzHoK1PEDJoyvGduafjs0sgYbG5Av7ib+Y+irFPnHibDZ+ahBJ3Q93v7/rZjLwZ9XDd79e04CHTMXvBf+s+sa94BfYuMv0VsX9veH9CnCJJCqK2ZS+aybs6jzRtvp1HwtXfmjZwv/bQsfYdlJEP/QxgrQHUZ1T3UJ66qDeT3lzBjAt7cWmsDW3aW+dA7gEYds9xzVGNrrwIno8yx7v3V7j4Hehfz5BGIc5gzWLuI6XUBOA1wBV4X2v93DHvvwKMsix6A2201gH2jMkplRfBsv+a14lLTfv2RW+bpoJeF9VadXNSrnV+fIBZtwzl7OcWA2a65v9e3o8dKfncO9s8U/d/f4/Dw82FjY+Ns/3O3Kac7sDDx7RR7/3VLId0b7rPFqIFsltSUEq5Am8B44BkYK1Sap7WeseRdbTW99RY/w6g4akZxcmL/91MSXHdj2Z0ilur427CSi8o5Y3f41m6J53MgnKGdw3h5Sv60cbPi0sGdGDxrnTevWYgHYO86dHOn0GRQWQUllnvKG7UqRoaW0BH03nr4QttGr7ZTQhnZs+awmAgXmudCKCUmgVcBOyoZ/0rgRY2+LiFOPCn6T/odHatjsQvVx9kRXwGVw2O4NOV+1m4w8z9/+CEHkwb2cW63otT+qKUqjW1Q8cgbzoGtZCJ2c6abr5Peu2U7wYVwlnYMyl0AGo+gy4ZGFLXikqpCCAKWGzHeJzXofXQfgBVypWEtAJ2pxbwwq+7rA+D2Z1aQI7lubpArccjAtbn7LZY7fvDZe87OgohWgR7JoW62hPq69WeCszR+shsUsfsSKlbgFsAOnXq1DjROYvqKnPX7aCbeHXRHt5YHI+bi8LNVeGiIKa9eZrXEb6ebvQLb/gpUEKIM5M9k0IyUHOSmXAgpZ51pwL1PulCa/0e8B6Y0UeNFaBTyD0AlaVkeEXx/qJ9dA71wcfDjXeuicXNxYXsonLun7OZEdGh3Dg8Ci93l+bdPyCEsCt7JoW1QLRSKgo4hDnxHzcWUCnVHQgEVtoxFudTVWnuBD28BYBP9rrj5qL44qYhhLU+Ol6+XWsvfr7z1B/cIYQ4s9itsVhrXQlMB34FdgKztdbblVJPKaVqzkp2JTBLt7QbJpqL8mIzc2X6TnNn6pGyp4Phj+etM3x+kdiKqYM71koIQghxLLvep6C1ng/MP6ZsxjHLT9gzhjNadTV8MN487AbMBHVD/nl0aoClzwJwsMeN5Gzy4ZxuofXsSAghDAdOdCJOS0UJzPnH0YQAkLHTPMSkhjKf9vxt9yh8Pd0YGFHPnPFCCGEhSaElKiuANweZqYv7XWnmoN85zzwPWblCcBd+73ArM7aEEtqqFWlZ+Xx0wwC8PeTXLYQ4MTlLtCQlOWYmyax4kxAGXGMSApippGMuAczjJ+96YiGFZRUcKqjgzjHRjOou888LIRomSaEl+eJv5nGMbWLM8rkP1bnahoO5FJZV8vikXoyPaUeY/yk8DlII4ZQkKbQUJTkmIcDRZ+H6m4eh7EjJx8fTlfBAb95cHM9nqw7g6+nGlIHh+Hm5OyhgIURLJEmhpVj2EqDMMwgO/GmejeDiQkpuCRNfN08s8/Fwpai8ip5h/rw4pa8kBCHESZOk0BJUV8Fmy/N8O8SaL4tViVnW10XlVTxzSW+uGtxJ7koWQpwSSQotQdJqKM6EXpOPe2vRzjQ8XF2ICvHhgQndGdOzrQMCFEKcKSQpNHflxfDtTeaRmV3HWYszC8u4a9ZG/ozPYtrILjw4QZ4TIIQ4fS18TmQnsH8F5B8yCcHL31r86qI9/BmfxT+GRXHXmGgHBiiEOJNITaG5y9xtvl/0Zq3iDQdyGREdwoxJvRwQlBDiTCU1heasugrWfQTeweAdZC2uqKomPr2QXmH+J9hYCCFOniSF5uzP1yA7AToMtBaVlFdx+f9WUl5VTUwHeRiOEKJxSVJornKTzNTXXcfC1K+sxR//tZ+NB3MBGN41xFHRCSHOUNKn0NyUF0NhKuyYB5WlMPElcDW/pt2pBby6aA8Afx8aQZCPhyMjFUKcgSQpNCcVJfDWYMhLAk9/aNcXgqIAc5Pa9C834Oflzoq7RhDq5+ngYIUQZyJJCs1J/CKTEADK8iFiGAApuSXc8NFaqqo1H98wSBKCEMJuJCk0Jzn7ay9bOpi/XZ9MSUUVy+4fRadg76aPSwjhNCQpNCc5B8CzNYx5DBKXQtcxbDyYw2u/7+XsLsGSEIQQdidJoTnJPQiBnWDwzeYLeOT75VRWa+6Uu5aFEE1AhqQ2Jzn7ICDCuphZWMaOw/ncNSaaoZ2DHRiYEMJZSE2hOVj2EiSvg8w90ONCAMorq3npVzPFxege8ihNIUTTkKTgaPuWw+Knjy6HdAPggTmbmbspBYDecueyEKKJSPORI1WWw8JHwN3naFkbMwX22v05ANw3vhuuLvLAHCFE05CagiNtmQWHN0PMJXDO/Wb0UVh/EjMKOZRbwr3jujF9tHQwCyGajiQFR0paY75f/C64e0HbGL5dn8z/fbMZgMgQnxNsLIQQjU+SgiMd2gBdx7L6fbmfAAAgAElEQVRsXwFLdyfi5e7CtxuSAWjr70lspwAHByiEcDaSFBylvAidsZO5pQO458M11uJQP0/mTR9G33BJCEKIpmfXpKCUmgC8BrgC72utn6tjncuBJwANbNZaX2XPmJqNw5tRupp5mWEA3H9ed6ad2wUX6VQWQjiQ3ZKCUsoVeAsYByQDa5VS87TWO2qsEw08DAzTWucopZxmQH5Vwh8oFG4d4/h24mAGRgQ1vJEQQtiZPWsKg4F4rXUigFJqFnARsKPGOjcDb2mtcwC01ul2jKd5iF8EP9+HzkthfXU3rhoVKwlBCNFs2PM+hQ5AUo3lZEtZTd2AbkqpP5VSqyzNTWeu0jyYezvVwA9VZ7Mo7FZGdg91dFRCCGFlz5pCXY3juo7PjwZGAuHAcqVUb611bq0dKXULcAtAp06dGj/SprL5ayhM5efBn/N/y1z4+ryhKCV9CEKI5sOeNYVkoGON5XAgpY51ftBaV2it9wG7MUmiFq31e1rrOK11XGhoy72y1lnxFNGKO5YpRkSHMDhKmo2EEM2LPZPCWiBaKRWllPIApgLzjllnLjAKQCkVgmlOSrRjTA5VlL6Pg9UhgOKxC3tJLUEI0ezYLSlorSuB6cCvwE5gttZ6u1LqKaXUZMtqvwJZSqkdwBLgfq11lr1icqTV69eRnriFZB3KT3cMp1tbP0eHJIQQx7HrfQpa6/nA/GPKZtR4rYF7LV9nrqpKev80GR+XInb6nyOzngohmi2ZJbUJVKRux0cXMcf/Wsbc+rKjwxFCiHpJUmgCB7YsB6DdsL/j5S3NRkKI5kuSgp1VVFaSs3EeefgQ23+Ao8MRQogTkqRgZ/s/vIFB5atx9w7A29Pd0eEIIcQJSVKwp/1/Ep1iRuF6D7/NwcEIIUTDJCnYi9ZU/ngPSTqU1wcthrOnOzoiIYRokCQFeylIxS1rNx9Uns/5cd0cHY0QQthEkoK9ZO4GIMu7C9Fyo5oQooWQpGAvGXsA8Arr4eBAhBDCdvI4TnuoLKNq9f9Irm5Dx06dHR2NEELYrMGaglJqulIqsCmCOWOs+wjX7HhmVN5A73CZ0kII0XLY0nzUDvMozdlKqQlKpvZs2OHNFHq25Y/qfjLPkRCiRWkwKWitH8U84+AD4Hpgr1LqP0qpLnaOrcUqzUhkV2kQg6OCaOPn5ehwhBDCZjZ1NFtmM021fFUCgcAcpdQLdoytxSrJ2MchQnjliv6ODkUIIU6KLX0Kdyql1gMvAH8CfbTW04CBwGV2jq/FqS7Mwr8iA/fgSDoEtHJ0OEIIcVJsGX0UAlyqtT5Qs1BrXa2UutA+YbVcW757jj5a49VP8qUQouWxpfloPpB9ZEEp5aeUGgKgtd5pr8Baoj1pBSTv3cxB2jH0rBGODkcIIU6aLUnhHaCwxnKRpUwc44Pl++ioMgntGI23h9wCIoRoeWxJCsrS0QyYZiPkprfjFJZV8vPWw3Rxz8S3rQzMEkK0TLYkhURLZ7O75esuINHegbU0X60+iC4rwLcqDwI6OTocIYQ4JbYkhVuBs4FDQDIwBLjFnkG1RL/vSmNsmwKzENzVscEIIcQparAZSGudDkxtglharNKKKjYezOX56BzIB0K7OzokIYQ4JQ0mBaWUF3AjEANYb8/VWv/DjnG1KIt3pVNWWU2sdzooVwiMcnRIQghxSmxpPvoMM//RecAfQDhQYM+gWpqF21MJ8fWgY+leCO0Bbh6ODkkIIU6JLUmhq9b6MaBIa/0JcAHQx75htSybk/OI7RiASlkPHWIdHY4QQpwyW5JCheV7rlKqN9AaiLRbRC3Mvswi9mUWMSK0EEpyoMNAR4ckhBCnzJb7Dd6zPE/hUWAe4As8ZteoWpBXftuDn6cbFwYdNgWSFIQQLdgJk4JSygXI11rnAMsAeYxYDZVV1Szdnc55vdsRmPItuLWCNj0dHZYQQpyyEzYfWe5enn6qO7c8lGe3UipeKfVQHe9fr5TKUEptsnzddKqf5Qgr4jPJL61kUqdy2PI1DLoRXN0dHZYQQpwyW5qPflNK3Qd8jZn3CACtdXb9m4BSyhV4CxiHueltrVJqntZ6xzGrfq21PuXE4wjr9mcze10Ss9clE+DtzrCKleaNwTc7NjAhhDhNtiSFI/cj3F6jTNNwU9JgIF5rnQiglJoFXAQcmxRalINZxVzx3iqqqs10UGd1DsZtz3xo2wcCIx0bnBBCnCZbHscZVceXLX0LHYCkGsvJlrJjXaaU2qKUmqOU6ljXjpRStyil1iml1mVkZNjw0fazZn82VdWaObeexTVDO/HIyLZwcBX0uMChcQkhRGOw5Y7ma+sq11p/2tCmdW12zPKPwFda6zKl1K3AJ8DoOj7rPeA9gLi4uGP30aQ2J+Xi6+nGgE6BxEUGwd7fAA1R8vwEIUTLZ0vz0aAar72AMcAGoKGkkAzUvPIPB1JqrqC1zqqxOBN43oZ4HGp3WgHd2/nh6mLJeclrQblAmDyPWQjR8tkyId4dNZeVUq0xU180ZC0QrZSKwsywOhW46ph9hWmtLQP8mQw0+ye57cssYmS3ULNQVgBr34dOZ4Onr2MDE0KIRnAqD8spBqIbWklrXamUmg78CrgCH2qttyulngLWaa3nAXcqpSYDlZhHfl5/CvE0mcKySjIKyogK9TEFexdCcRaMetixgQkhRCOxpU/hR472BbgAvYDZtuxcaz0f84znmmUzarx+GGgxZ9T9mWZEbmSwJSkkLIFWgdDpLAdGJYQQjceWmsJLNV5XAge01sl2iqdZS8ktAaBDQCtTkJ9ihqG6uDouKCGEaES2JIWDwGGtdSmAUqqVUipSa73frpE1Q4fzSgEIC7A8VqIoHfzaOzAiIYRoXLbMkvoNUF1jucpS5nRS8kpwd1WE+HiagsIM8G3j2KCEEKIR2ZIU3LTW5UcWLK+d8ikyh3NLadfaCxcXBdXVUCRJQQhxZrElKWRYRggBoJS6CMi0X0jNV2ZhGaG+llrC3l9BV4GPJAUhxJnDlj6FW4EvlFJvWpaTgTrvcj7T5RRX0OFIf8L6T8z38EH1byCEEC2MLTevJQBDlVK+gNJaO+3zmXOKyund3t8s5CVBtwkQLg/VEUKcORpsPlJK/UcpFaC1LtRaFyilApVS/26K4JoTrTXZxeUE+XiA1pB7EAIiHB2WEEI0Klv6FM7XWuceWbA8hW2i/UJqnkoqqiivrCbQxwNKc6EsHwI6OTosIYRoVLYkBVellOeRBaVUK8DzBOufkbKLzACsIG8PyNhtCoO7ODAiIYRofLZ0NH8O/K6U+siyfANmimunklVokkJbl1z48DxT2H6AAyMSQojGZ0tH8wtKqS3AWMwzEn4BnK4xPSmnGIDu2YuPFvq1c1A0QghhH7Y0HwGkYu5qvgzzPIVmP8V1YzuQZZJCSNpf4O4Dt69xcERCCNH46q0pKKW6YZ6BcCWQBXyNGZI6qolia1aSsosJ8fXALXsPRI+D0O6ODkkIIRrdiWoKuzC1gkla6+Fa6zcw8x45pa2H8uge4gE5+yUhCCHOWCdKCpdhmo2WKKVmKqXGUPdzl894KbklbE/JZ3J4MehqCOnm6JCEEMIu6k0KWuvvtdZXAD2ApcA9QFul1DtKqfFNFF+z8PvONABG+lkeIxHWz4HRCCGE/TTY0ay1LtJaf6G1vhAIBzYBD9k9smZk4Y40okJ8aJO/AzxbQ5DcnyCEODPZOvoIAK11ttb6f1rr0fYKqLkpKK1gVWIWY3u2QaVtg3Z9wOWkfmxCCNFiyNmtAX/syaCiSjOuZ1vI3AOh0p8ghDhzSVJowKIdaQR6uxMbXGHmPJJOZiHEGUySwglUVFWzeFc6o3u0xS1vvykM7urQmIQQwp4kKZzAD5tSyC+tZFyvtpCfYgr92zs2KCGEsCNJCifw/vJEenfwZ2zPNlCQagr9whwblBBC2JEkhXoUl1eyJ62A0d3b4ObqAgUp4OYFrQIdHZoQQtiNJIV6bE/Jp1pD3/AAU5B/2NQSlFPe1C2EcBKSFOqx63A+AL3a+5vHbyavkTmPhBBnPLsmBaXUBKXUbqVUvFKq3ruglVJTlFJaKRVnz3hOxq7UAvy83Ahr7QWpW80zmXtc4OiwhBDCruyWFJRSrsBbwPlAL+BKpVSvOtbzA+4EVtsrllOxJ62A7m39UErBrp9AuUC38x0dlhBC2JU9awqDgXitdaLWuhyYBVxUx3pPAy8ApXaM5aRordmdWkD3dn6mIGExhA8C31DHBiaEEHZmz6TQAUiqsZxsKbNSSg0AOmqtf7JjHCctNb+U/NJKkxQqy+HwFug42NFhCSGE3dkzKdQ1TEdb31TKBXgF+L8Gd6TULUqpdUqpdRkZGY0YYt12pxYA0K2tH6Rvh6oyaB9r988VQghHs2dSSAY61lgOB1JqLPsBvYGlSqn9wFBgXl2dzVrr97TWcVrruNBQ+zfhxKcXApakcGi9Keww0O6fK4QQjmbPpLAWiFZKRSmlPDDPe5535E2tdZ7WOkRrHam1jgRWAZO11uvsGJNN4tMLCfbxIMjHAw5tAO8QCOjk6LCEEMLu7JYUtNaVwHTgV2AnMFtrvV0p9ZRSarK9Prcx7E0vpGsbX7OQvhPa9Zab1oQQTsHNnjvXWs8H5h9TNqOedUfaMxZbaa3Zm1bA5P6Wie9yD0DPSY4NSgghmojc0XyMjIIy8ksriW7jB2WFUJwlTUdCCKchSeEY//hkLYBpPso9aAoDIhwYkRBCNB1JCjVsOJjDtkP5nNstlMFRQbB2prmTOay/o0MTQogmIUmhhqW7M3BR8MZVA3DP2g3rPoSht0GIPG1NCOEcJClY5BaX8/3GZHq198ffyx2STTMSg25ybGBCCNGEJClYPDJ3G6l5pdwxOtoUZOw2D9WRTmYhhBORpABUVlXz+840rhzcifNi2pnCzD0Q3BVcXB0bnBBCNCFJCsCetEJKK6oZGGF51GZRFhxYCe0HODYwIYRoYna9ea2l2Gl5ylpMOx9Y8BCsfse8ITetCSGcjNQUgOScEgAislccTQgAUec6KCIhhHAMqSkAyTnFtPHzxD1vtymIHg/B0eDu5djAhBCiiUlSAA7llhAe2AryDoG7D1w1WybAE0I4JWk+Ag5kFRMe6A15SdC6gyQEIYTTcvqkkFNUzqHcEnq194f8Q9A63NEhCSGEwzh9UtieYkYe9W7fGvKSwb9DA1sIIcSZy+mTwraUPABi2npBYbrUFIQQTs3pk8LWQ3mEB7YisCoT0JIUhBBOTZJCcp5pOto8yxRI85EQwok5dVJIyy/lYHYxcREBsOY9U9imp2ODEkIIB3LqpLBmXzYAw9qUmcduTngO/No5OCohhHAcp04Ke9IKcFHQtXyHKQgf5NiAhBDCwZw6KSRkFBIR7IP73l+hVZA8dlMI4fScOymkF9El1AcOroLOI8FVZv0QQjg3p00KlVXV7MssokuoNxSkQGCEo0MSQgiHc9qkkJxTQnlVNb1al0N1Jfi1d3RIQgjhcE6bFBIyCgGI9jLf8Q9zYDRCCNE8OG1SiE83ySDCLccUSE1BCCGcNykkZBQS4uuJz7bPwdMfQro6OiQhhHA4uyYFpdQEpdRupVS8UuqhOt6/VSm1VSm1SSm1QinVy57x1LQrtYAeIe6QsBgGXgderZvqo4UQotmy2xhMpZQr8BYwDkgG1iql5mmtd9RY7Uut9buW9ScDLwMT7BXTEUt2p7MlOY8nYoshtQLCB9v7I4UQ9aioqCA5OZnS0lJHh3JG8PLyIjw8HHd391Pa3p4D8wcD8VrrRACl1CzgIsCaFLTW+TXW9wG0HeOxWp1opre4qm2SiUbuZBbCYZKTk/Hz8yMyMhIlTz08LVprsrKySE5OJioq6pT2Yc/mow5AUo3lZEtZLUqp25VSCcALwJ12jMdqf6a5ac0j/hdzF7OMPBLCYUpLSwkODpaE0AiUUgQHB59WrcueSaGu3/BxNQGt9Vta6y7Ag8Cjde5IqVuUUuuUUusyMjJOO7B9mUVEhfhC+i7oKE1HQjiaJITGc7o/S3smhWSgY43lcCDlBOvPAi6u6w2t9Xta6zitdVxoaOhpBaW15mB2MVEBrlCWB75tTmt/QoiWLTc3l7fffvukt5s4cSK5ubknXGfGjBksWrToVENzCHsmhbVAtFIqSinlAUwF5tVcQSkVXWPxAmCvHeMBIL+0kpKKKjp7FZkCH0kKQjiz+pJCVVXVCbebP38+AQEBJ1znqaeeYuzYsacVX1OzW1LQWlcC04FfgZ3AbK31dqXUU5aRRgDTlVLblVKbgHuB6+wVzxFp+aatrYOH5U5mqSkI4dQeeughEhIS6N+/P4MGDWLUqFFcddVV9OnTB4CLL76YgQMHEhMTw3vvvWfdLjIykszMTPbv30/Pnj25+eabiYmJYfz48ZSUlABw/fXXM2fOHOv6jz/+OLGxsfTp04ddu3YBkJGRwbhx44iNjeWf//wnERERZGZmNvFP4Si7TguqtZ4PzD+mbEaN13fZ8/PrkppnkkI7V8vAJ6kpCNFsPPnjdnak5De84kno1d6fxyfF1Pv+c889x7Zt29i0aRNLly7lggsuYNu2bdbROx9++CFBQUGUlJQwaNAgLrvsMoKDg2vtY+/evXz11VfMnDmTyy+/nG+//ZZrrrnmuM8KCQlhw4YNvP3227z00ku8//77PPnkk4wePZqHH36YX375pVbicQSnu6M51VJTCKnOMgVSUxBC1DB48OBawzlff/11+vXrx9ChQ0lKSmLv3uNbuaOioujf3zyPZeDAgezfv7/OfV966aXHrbNixQqmTp0KwIQJEwgMDGzEozl5TvcAgeV7Mwnwdicgb6e5i7l1uKNDEkJYnOiKvqn4+PhYXy9dupRFixaxcuVKvL29GTlyZJ3DPT09Pa2vXV1drc1H9a3n6upKZWUlYAa/NCdOVVOorKpm0Y40JvYJwyVlA3QYCDIUTgin5ufnR0FBQZ3v5eXlERgYiLe3N7t27WLVqlWN/vnDhw9n9uzZACxcuJCcnJxG/4yT4VRJYU9aISUVVYxqUwhpWyHibEeHJIRwsODgYIYNG0bv3r25//77a703YcIEKisr6du3L4899hhDhw5t9M9//PHHWbhwIbGxsSxYsICwsDD8/Pwa/XNspZpb1aUhcXFxet26dae07ddrD/Kfb1ey0fcuXCpL4LZV0KZnI0cohDgZO3fupGdP5/0/LCsrw9XVFTc3N1auXMm0adPYtGnTae2zrp+pUmq91jquoW2dqk8hIaOIS91XmoRwwcuSEIQQDnfw4EEuv/xyqqur8fDwYObMmQ6Nx6mSwsGsYoa1KoIKFxh4g6PDEUIIoqOj2bhxo6PDsHKqPoWD2cWEexRBqyBwcapDF0IImzjNmTE5p5iEjELauhWBd3DDGwghhBNymuaj7zccwt3Vhc4+ZeAW4uhwhBCiWXKamsL00V2Zf+cIvMpzwTvI0eEIIUSz5DRJQSlFp2BvKMowfQpCCHEKfH19AUhJSWHKlCl1rjNy5EgaGjr/6quvUlxcbF22ZSrupuA0SQGAwgwozoTgro6ORAjRwrVv3946A+qpODYp2DIVd1NwrqSQssF8D2/w/g0hhJN48MEHaz1P4YknnuDJJ59kzJgx1mmuf/jhh+O2279/P7179wagpKSEqVOn0rdvX6644opacx9NmzaNuLg4YmJiePzxxwEzyV5KSgqjRo1i1KhRwNGpuAFefvllevfuTe/evXn11Vetn1ffFN2NyWk6mgFI3Wq+t+vj2DiEEHVb8NDR/9PG0q4PnP9cvW9PnTqVu+++m9tuuw2A2bNn88svv3DPPffg7+9PZmYmQ4cOZfLkyfU+6vKdd97B29ubLVu2sGXLFmJjY63vPfPMMwQFBVFVVcWYMWPYsmULd955Jy+//DJLliwhJKT2wJf169fz0UcfsXr1arTWDBkyhHPPPZfAwECbp+g+Hc5VU8g9CD6h4Om4eUWEEM3LgAEDSE9PJyUlhc2bNxMYGEhYWBj/+te/6Nu3L2PHjuXQoUOkpaXVu49ly5ZZT859+/alb9++1vdmz55NbGwsAwYMYPv27ezYseOE8axYsYJLLrkEHx8ffH19ufTSS1m+fDlg+xTdp8O5agq5ByGgk6OjEELU5wRX9PY0ZcoU5syZQ2pqKlOnTuWLL74gIyOD9evX4+7uTmRkZJ1TZtdUVy1i3759vPTSS6xdu5bAwECuv/76BvdzovnobJ2i+3Q4WU3hgCQFIcRxpk6dyqxZs5gzZw5TpkwhLy+PNm3a4O7uzpIlSzhw4MAJtz/nnHP44osvANi2bRtbtmwBID8/Hx8fH1q3bk1aWhoLFiywblPflN3nnHMOc+fOpbi4mKKiIr7//ntGjBjRiEd7Ys5TUyjOhpwD0LvuIWRCCOcVExNDQUEBHTp0ICwsjKuvvppJkyYRFxdH//796dGjxwm3nzZtGjfccAN9+/alf//+DB48GIB+/foxYMAAYmJi6Ny5M8OGDbNuc8stt3D++ecTFhbGkiVLrOWxsbFcf/311n3cdNNNDBgwwC5NRXVxnqmzN30Fc2+FmxZD+MDGD0wIcUqcfepsezidqbOdp/nIqzX0uBDaD3B0JEII0Ww5T/NRj4nmSwghRL2cp6YghBCiQZIUhBAO19L6Npuz0/1ZSlIQQjiUl5cXWVlZkhgagdaarKwsvLy8TnkfztOnIIRolsLDw0lOTiYjI8PRoZwRvLy8CA8PP+XtJSkIIRzK3d2dqKgoR4chLKT5SAghhJUkBSGEEFaSFIQQQli1uGkulFIZwIlnp6pfCJDZiOE40plyLGfKcYAcS3Mlx2JEaK1DG1qpxSWF06GUWmfL3B8twZlyLGfKcYAcS3Mlx3JypPlICCGElSQFIYQQVs6WFN5zdACN6Ew5ljPlOECOpbmSYzkJTtWnIIQQ4sScraYghBDiBJwiKSilJiildiul4pVSDzk6noYopT5USqUrpbbVKAtSSv2mlNpr+R5oKVdKqdctx7ZFKRXruMiPp5TqqJRaopTaqZTarpS6y1Le4o5HKeWllFqjlNpsOZYnLeVRSqnVlmP5WinlYSn3tCzHW96PdGT8x1JKuSqlNiqlfrIst9Tj2K+U2qqU2qSUWmcpa3F/XwBKqQCl1Byl1C7L/8xZTX0sZ3xSUEq5Am8B5wO9gCuVUr0cG1WDPgYmHFP2EPC71joa+N2yDOa4oi1ftwDvNFGMtqoE/k9r3RMYCtxu+fm3xOMpA0ZrrfsB/YEJSqmhwPPAK5ZjyQFutKx/I5Cjte4KvGJZrzm5C9hZY7mlHgfAKK11/xrDNVvi3xfAa8AvWuseQD/M76dpj0VrfUZ/AWcBv9ZYfhh42NFx2RB3JLCtxvJuIMzyOgzYbXn9P+DKutZrjl/AD8C4ln48gDewARiCuZnI7di/N+BX4CzLazfLesrRsVviCcecYEYDPwGqJR6HJab9QMgxZS3u7wvwB/Yd+7Nt6mM542sKQAcgqcZysqWspWmrtT4MYPnexlLeYo7P0uwwAFhNCz0eS5PLJiAd+A1IAHK11pWWVWrGaz0Wy/t5QHDTRlyvV4EHgGrLcjAt8zgANLBQKbVeKXWLpawl/n11BjKAjyzNeu8rpXxo4mNxhqSg6ig7k4ZctYjjU0r5At8Cd2ut80+0ah1lzeZ4tNZVWuv+mCvtwUDPulazfG+Wx6KUuhBI11qvr1lcx6rN+jhqGKa1jsU0p9yulDrnBOs252NxA2KBd7TWA4AijjYV1cUux+IMSSEZ6FhjORxIcVAspyNNKRUGYPmebilv9senlHLHJIQvtNbfWYpb7PEAaK1zgaWYfpIApdSRZ5PUjNd6LJb3WwPZTRtpnYYBk5VS+4FZmCakV2l5xwGA1jrF8j0d+B6TrFvi31cykKy1Xm1ZnoNJEk16LM6QFNYC0ZaRFR7AVGCeg2M6FfOA6yyvr8O0zR8pv9YyEmEokHekqtkcKKUU8AGwU2v9co23WtzxKKVClVIBltetgLGYjsAlwBTLascey5FjnAIs1pbGX0fSWj+stQ7XWkdi/h8Wa62vpoUdB4BSykcp5XfkNTAe2EYL/PvSWqcCSUqp7paiMcAOmvpYHN250kQdOBOBPZj230ccHY8N8X4FHAYqMFcDN2LacH8H9lq+B1nWVZjRVQnAViDO0fEfcyzDMVXaLcAmy9fElng8QF9go+VYtgEzLOWdgTVAPPAN4Gkp97Isx1ve7+zoY6jjmEYCP7XU47DEvNnytf3I/3dL/PuyxNcfWGf5G5sLBDb1scgdzUIIIaycoflICCGEjSQpCCGEsJKkIIQQwkqSghBCCCtJCkIIIawkKQhhoZSqssy0eeSr0WbUVUpFqhqz3grRXLk1vIoQTqNEmykshHBaUlMQogGW+fqfV+ZZCmuUUl0t5RFKqd8tc9n/rpTqZClvq5T6XpnnLmxWSp1t2ZWrUmqmMs9iWGi5Kxql1J1KqR2W/cxy0GEKAUhSEKKmVsc0H11R4718rfVg4E3MPEFYXn+qte4LfAG8bil/HfhDm+cuxGLutAUz7/1bWusYIBe4zFL+EDDAsp9b7XVwQthC7mgWwkIpVai19q2jfD/m4TqJlsn9UrXWwUqpTMz89RWW8sNa6xClVAYQrrUuq7GPSOA3bR6UglLqQcBda/1vpdQvQCFmWoO5WutCOx+qEPWSmoIQttH1vK5vnbqU1XhdxdE+vQswc9gMBNbXmKlUiCYnSUEI21xR4/tKy+u/MLOMAlwNrLC8/h2YBtaH8vjXt1OllAvQUWu9BPPQmwDguNqKEE1FrkiEOKqV5alqR/yitT4yLNVTKbUacyF1paXsTuBDpdT9mCdm3WApvwt4Tyl1I6ZGMA0z621dXIHPlVKtMbNevqLNsxqEcAjpUxCiAZY+hTitdaajYxHC3qT5SAghhFPNaPUAAAAzSURBVJXUFIQQQlhJTUEIIYSVJAUhhBBWkhSEEEJYSVIQQghhJUlBCCGElSQFIYQQVv8PnJnsaaZH8ZsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 訓練セットと検証セットの学習サイクルを図で表す\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "#plt.plot(range(1, epochs+1), result.history['acc'], label=\"training\")\n",
    "#plt.plot(range(1, epochs+1), result.history['val_acc'], label=\"validation\")\n",
    "plt.plot(epochs, acc, label=\"training\")\n",
    "plt.plot(epochs, val_acc, label=\"validation\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 損失関数に対してのグラフ\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "\n",
    "plt.plot(epochs, loss, label='loss')\n",
    "plt.plot(epochs, val_loss, label='val_loss')\n",
    "#plt.ylim((0.2, 0.7))\n",
    "plt.legend(loc='best')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多クラスの混合行列を作成\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.2270360e-03, 1.1877749e-02, 5.9570479e-01, 3.6558533e-01,\n",
       "        2.4605026e-02],\n",
       "       [9.8987684e-06, 1.1612633e-05, 5.6407214e-03, 1.9961463e-01,\n",
       "        7.9472315e-01],\n",
       "       [2.7072988e-07, 1.4754943e-07, 1.6743597e-04, 5.0497510e-02,\n",
       "        9.4933468e-01],\n",
       "       ...,\n",
       "       [4.0762071e-04, 7.0384485e-03, 4.5925874e-01, 5.2662283e-01,\n",
       "        6.6723619e-03],\n",
       "       [7.6357217e-08, 2.6260764e-08, 6.8762805e-05, 3.9953306e-02,\n",
       "        9.5997787e-01],\n",
       "       [9.0103247e-04, 7.1773916e-03, 3.4658414e-01, 6.2552196e-01,\n",
       "        1.9815473e-02]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-e034b31e9b97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.argmax(y_test, axis=1), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-208f22876396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "#多クラスの混合行列を作成\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred = model.predict(X_test)\n",
    "#print(confusion_matrix(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6135104906504224\n",
      "Test accuracy: 0.718223583734784\n"
     ]
    }
   ],
   "source": [
    "# score = model.evaluate(X_test, y_test, verbose = 0)\n",
    "# model.evaluate()は、最終的評価（損失関数と精度）を出すため，X_test，y_testを入れるべきだと思う\n",
    "# たぶんここでの、epoch数は、model.fit()で宣言したepoch数と同じになるのかな？\n",
    "score = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-208f22876396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "#多クラスの混合行列を作成\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
